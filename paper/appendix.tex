\section{Full taxonomy details}
\label{app:taxonomy}

\cref{tab:taxonomy_full} provides the complete taxonomy with BibTeX examples for each hallucination type.

\begin{table}[h]
\caption{Full taxonomy with example BibTeX snippets illustrating each hallucination type. Red text indicates the hallucinated field.}
\label{tab:taxonomy_full}
\centering
\small
\begin{tabular}{clp{8cm}}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{Example (hallucinated field in red)} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi} & \texttt{doi = \{\textcolor{red}{10.9999/nips2024.1847}\}} \\
& \texttt{nonexistent\_venue} & \texttt{booktitle = \{\textcolor{red}{Intl.\ Conf.\ on Advanced AI Systems}\}} \\
& \texttt{placeholder\_authors} & \texttt{author = \{\textcolor{red}{John Doe and Jane Smith}\}} \\
& \texttt{future\_date} & \texttt{year = \{\textcolor{red}{2030}\}} \\
\midrule
\multirow{5}{*}{2}
& \texttt{chimeric\_title} & Real authors, \texttt{title = \{\textcolor{red}{A Novel Approach...}\}} (nonexistent) \\
& \texttt{wrong\_venue} & Real paper, \texttt{booktitle = \{\textcolor{red}{ICML}\}} (actually NeurIPS) \\
& \texttt{author\_mismatch} & Real title, \texttt{author = \{\textcolor{red}{Wrong Author List}\}} \\
& \texttt{preprint\_as\_published} & arXiv paper, \texttt{booktitle = \{\textcolor{red}{NeurIPS}\}} (never published) \\
& \texttt{hybrid\_fabrication} & Valid DOI resolves, but \texttt{title = \{\textcolor{red}{...}\}} doesn't match \\
\midrule
\multirow{2}{*}{3}
& \texttt{near\_miss\_title} & \texttt{title = \{Attention Is All You \textcolor{red}{Want}\}} (vs.\ ``Need'') \\
& \texttt{plausible\_fabrication} & Entirely fabricated, all fields realistic but nonexistent \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\scriptsize Stress}}
& \texttt{merged\_citation} & Authors from paper A, \texttt{title} from \textcolor{red}{paper B}, venue from C \\
& \texttt{partial\_author\_list} & Real paper, \texttt{author = \{\textcolor{red}{First and Last}\}} (middle dropped) \\
& \texttt{arxiv\_version\_mismatch} & arXiv preprint cited with \textcolor{red}{wrong venue} and shifted year \\
\bottomrule
\end{tabular}
\end{table}

\section{Real-world incident mapping}
\label{app:real-world-mapping}

We mapped 72 real-world hallucinated citations from three documented incident studies to our taxonomy.
\cref{tab:realworld} shows the mapping results.

\begin{table}[h]
\caption{Mapping of 72 real-world hallucinated citations to \textsc{Hallmark} taxonomy types. Citations were sourced from GPTZero's NeurIPS 2025 analysis, GhostCite, and HalluCitation.}
\label{tab:realworld}
\centering
\small
\begin{tabular}{llr}
\toprule
\textbf{Taxonomy type} & \textbf{Tier} & \textbf{Count} \\
\midrule
\texttt{plausible\_fabrication} & 3 & 40 \\
\texttt{fabricated\_doi} & 1 & 10 \\
\texttt{chimeric\_title} & 2 & 8 \\
\texttt{near\_miss\_title} & 3 & 5 \\
\texttt{wrong\_venue} & 2 & 4 \\
\texttt{author\_mismatch} & 2 & 3 \\
\texttt{hybrid\_fabrication} & 2 & 2 \\
\midrule
\textbf{Total mapped} & & \textbf{72} \\
\bottomrule
\end{tabular}
\end{table}

\noindent Of the 72 real-world citations, 57 (79\%) map directly to existing taxonomy types.
The remaining 15 exhibit compound failure modes (e.g., fabricated DOI \emph{and} wrong venue simultaneously); we assigned these to the highest-tier applicable type.
The dominance of \texttt{plausible\_fabrication} (55\%) reflects the predominant LLM failure mode: models generate coherent but entirely fictional references rather than subtly corrupting real ones.
Three taxonomy types---\texttt{merged\_citation}, \texttt{partial\_author\_list}, and \texttt{arxiv\_version\_mismatch}---have zero real-world instances, motivating their designation as stress-test types (\cref{sec:taxonomy}).

\section{Dataset construction details}
\label{app:dataset}

\paragraph{DBLP scraping.}
Valid entries were scraped from the DBLP API (\texttt{dblp.org/search/publ/api}) using venue-specific queries for NeurIPS, ICML, ICLR, AAAI, and CVPR.
We retrieved BibTeX records, verified DOI resolution via CrossRef, and confirmed title existence in Semantic Scholar.
Entries failing any verification step were excluded.

\paragraph{Perturbation pipeline.}
Systematic perturbations follow deterministic rules per hallucination type:
\begin{itemize}
    \item \texttt{fabricated\_doi}: Replace DOI with a non-resolving DOI using one of 20 non-existent prefixes and four suffix styles (path, identifier, year-indexed, and conference-indexed) to avoid template-detectable patterns.
    \item \texttt{nonexistent\_venue}: Replace venue with an LLM-generated plausible but nonexistent conference name.
    \item \texttt{placeholder\_authors}: Replace author list with common placeholder names.
    \item \texttt{future\_date}: Set year to current year + 5.
    \item \texttt{chimeric\_title}: Keep authors from paper A, replace title with LLM-generated plausible title.
    \item \texttt{wrong\_venue}: Keep all fields but swap venue with a different real venue.
    \item \texttt{author\_mismatch}: Keep title and venue, replace authors with those from a different paper.
    \item \texttt{preprint\_as\_published}: Take an arXiv-only paper and add a fabricated venue field.
    \item \texttt{hybrid\_fabrication}: Keep a valid DOI but replace title and authors with fabricated metadata.
    \item \texttt{near\_miss\_title}: Modify 1--2 words via six strategies: synonym substitution (POS-safe pairs), plural/singular flipping, British/American spelling swap, abbreviation expansion/contraction (e.g., ``RL'' $\leftrightarrow$ ``Reinforcement Learning''), hyphenation toggling (e.g., ``self-supervised'' $\leftrightarrow$ ``self supervised''), and article removal.
    \item \texttt{plausible\_fabrication}: LLM-generate a complete, realistic but nonexistent entry.
    \item \texttt{merged\_citation}: Combine metadata from 2--3 real papers into one entry (e.g., authors from paper A, title from paper B, venue from paper C).
    \item \texttt{partial\_author\_list}: Take a real paper and drop one or more middle co-authors, keeping only the first and last.
    \item \texttt{arxiv\_version\_mismatch}: Cite specific claims from superseded arXiv versions.
\end{itemize}

\paragraph{LLM-generated entries.}
We prompted GPT-5.1 to generate plausible but fictional citations for types requiring coherent fabrication (\texttt{plausible\_fabrication}, \texttt{chimeric\_title}, \texttt{fabricated\_doi}).
Each prompt requested a BibTeX entry with realistic metadata for a specified ML venue and year range.
Generated entries were verified against CrossRef and DBLP: entries with title similarity $\geq 85\%$ (token-sort ratio) and author Jaccard similarity $\geq 0.5$ to any real paper were flagged as potential duplicates of real work and excluded.
This filtering removed approximately 12\% of generated entries, documenting an LLM recall failure rate where the model reproduces real papers rather than fabricating new ones.
The remaining entries were assigned sub-test labels based on their hallucination type's expected failure pattern.

\paragraph{Real-world collection.}
We harvested 72 hallucinated citations from three documented incident studies: GPTZero's NeurIPS 2025 analysis, GhostCite~\citep{ghostcite2026}, and HalluCitation~\citep{hallucitation2026}.
Each entry was mapped to the closest taxonomy type based on its failure mode (e.g., a citation with a non-resolving DOI was classified as \texttt{fabricated\_doi}; a citation to a non-existent paper with plausible metadata was classified as \texttt{plausible\_fabrication}).
The real-world sample is type-skewed: 55\% are \texttt{plausible\_fabrication}, reflecting the predominant LLM failure mode in practice.
Types without real-world examples (\texttt{merged\_citation}, \texttt{partial\_author\_list}, \texttt{arxiv\_version\_mismatch}) are relegated to a separate stress-test split, evaluated independently from the main taxonomy.

\paragraph{Adversarial crafting.}
We manually constructed entries designed to evade specific detection strategies.
These include entries with DOIs that resolve to unrelated papers (\texttt{hybrid\_fabrication}), entries combining metadata from multiple real papers (\texttt{merged\_citation}), and entries with plausible but non-existent venues chosen to be close to real venue names.
Adversarial entries stress-test tool robustness beyond what template-based perturbation achieves.

\paragraph{Quality control.}
Every generated entry passes through automated validation:
(1)~BibTeX well-formedness check (all required fields present, valid syntax),
(2)~sub-test label consistency (sub-test ground truth matches the hallucination type's expected failure pattern),
(3)~cross-validation with the valid entry pool to prevent accidental duplicates.

\section{Core subset analysis}
\label{app:core-subset}

The hallucinated entries were scaled from 114 (71 in dev, 43 in test) to 816 (453 in dev, 363 in test) by generating additional entries across all main hallucination types.
To verify that this scaling does not distort evaluation signals, we re-evaluate all baselines on the original core subset containing only the 114 seed hallucinations alongside the 720 valid entries.
Aggregate metrics differ by less than 2\% between the core subset and the full dataset across all baselines: detection rate changes by at most 0.015, F1 by at most 0.02, and tier-weighted F1 by at most 0.018.
Per-type rankings are preserved---no baseline changes rank on any type.
We conclude that the scaled entries are consistent with the original distribution and do not introduce systematic bias.

\section{Full per-type results}
\label{app:pertype}

\cref{tab:pertype_full} reports detection rate for every hallucination type and baseline.

\begin{table}[h]
\caption{Per-type detection rate on \texttt{dev\_public} for independent tools. All main types have $\geq$30 instances. Stress-test types are evaluated in the separate \texttt{stress\_test} split.}
\label{tab:pertype_full}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llcccc}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{DOI} & \textbf{HaRC$^*$} & \textbf{v-cit$^*$} & \textbf{GPT-5.1} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi}        & 0.590 & --- & --- & 0.738 \\
& \texttt{nonexistent\_venue}     & 0.125 & --- & --- & 0.969 \\
& \texttt{placeholder\_authors}   & 0.235 & --- & --- & 0.941 \\
& \texttt{future\_date}           & 1.000 & --- & --- & 0.968 \\
\midrule
\multirow{5}{*}{2}
& \texttt{chimeric\_title}        & 0.143 & --- & --- & 0.857 \\
& \texttt{wrong\_venue}           & 0.200 & --- & --- & 0.829 \\
& \texttt{author\_mismatch}       & 0.206 & --- & --- & 0.632 \\
& \texttt{preprint\_as\_pub.}     & 0.129 & --- & --- & 0.839 \\
& \texttt{hybrid\_fabrication}    & 0.261 & --- & --- & 0.696 \\
\midrule
\multirow{2}{*}{3}
& \texttt{near\_miss\_title}      & 0.086 & --- & --- & 0.629 \\
& \texttt{plausible\_fabrication} & 0.036 & --- & --- & 0.821 \\
\bottomrule
\end{tabular}
\end{table}

\noindent All main hallucination types have $\geq$30 instances per public split.
Entries marked --- indicate insufficient coverage for per-type breakdowns (HaRC: 1.9\%, verify-citations: 6.6\%).
DOI-only with pre-screening (\cref{app:prescreening}) achieves non-zero detection across all types because the pre-screening layer catches entries via DOI resolution, year bounds, and author heuristics independently of the hallucination type.
Without pre-screening, DOI-only detects only \texttt{fabricated\_doi} entries.
GPT-5.1 achieves ${\geq}67\%$ detection on 9 of 11 main types, with near-perfect detection of \texttt{nonexistent\_venue} (0.969), \texttt{future\_date} (0.968), and \texttt{placeholder\_authors} (0.941).
Its two weakest types---\texttt{author\_mismatch} (0.632) and \texttt{near\_miss\_title} (0.629)---require exact bibliographic recall that even large LLMs lack.
Notably, DOI-only's deterministic year-bounds check achieves perfect detection on \texttt{future\_date} (1.000 vs.\ GPT-5.1's 0.968), suggesting that rule-based and LLM-based approaches are complementary.

\section{Statistical analysis}
\label{app:statistics}

\paragraph{Bootstrap confidence intervals.}
All aggregate metrics in \cref{tab:results} are accompanied by 95\% confidence intervals computed via stratified bootstrap with 10{,}000 resamples.
Stratification by hallucination type ensures that the resampled datasets preserve the original type distribution, preventing bootstrap bias from underrepresented types.

\paragraph{Significance testing.}
We use paired bootstrap tests~\citep{efron1994bootstrap} to compare tools pairwise.
For each pair of tools $(A, B)$, we resample entries with replacement and compute $\Delta = \text{F1}_A - \text{F1}_B$ for each resample.
The $p$-value is the fraction of resamples where $\Delta \leq 0$.

\paragraph{Per-type power analysis.}
With $n \geq 30$ instances per type per split, binary classification metrics have substantially narrower confidence intervals than the original 10-instance design.
For detection rate, a type with 30 hallucinated entries and 90\% true detection rate has a 95\% Clopper-Pearson confidence interval of $[0.74, 0.98]$, compared to $[0.55, 0.998]$ for $n=10$.
We report per-type results with improved statistical confidence, enabling more definitive type-level claims.
As the dataset grows through community contributions, per-type statistical power will continue to improve.

\paragraph{Tier weight sensitivity.}
We evaluate tier-weighted F1 under five weighting schemes: uniform $\{1,1,1\}$ (equivalent to standard macro-F1 across tiers), linear $\{1,2,3\}$ (default), quadratic $\{1,4,9\}$, log $\{\log(2), \log(3), \log(4)\}$, and inverse difficulty (weights proportional to $1 - \text{mean DR}$ across tools).
Tool rankings are stable across all schemes: GPT-5.1 achieves TW-F1 of 0.875 (uniform), 0.860 (linear), 0.850 (quadratic), 0.864 (log), and 0.865 (inverse difficulty).
bibtex-updater achieves 0.758, 0.753, 0.748, 0.755, and 0.752 under the same schemes.
DOI-only achieves 0.392, 0.310, 0.255, 0.337, and 0.342.
The relative ordering of tools (GPT-5.1 $>$ bibtex-updater $>$ DOI-only) is preserved under all weighting schemes, confirming that our conclusions are robust to the specific choice of tier weights.
GPT-5.1's narrow sensitivity range (0.850--0.875) reflects its consistently strong per-tier performance; DOI-only shows wider variation (0.255--0.392) because its detection is concentrated in Tier~1.

\section{Plackett-Luce ranking}
\label{app:ranking}

The Plackett-Luce model~\citep{plackett1975,luce1959} assigns a positive strength parameter $\theta_j$ to each tool $j \in \{1, \ldots, J\}$.
Given a ranking $\sigma$ over a subset $S$ of tools, the probability of observing $\sigma$ is:
\begin{equation}
P(\sigma \mid \boldsymbol{\theta}) = \prod_{k=1}^{|S|} \frac{\theta_{\sigma(k)}}{\sum_{l=k}^{|S|} \theta_{\sigma(l)}}.
\end{equation}

In \textsc{Hallmark}, we derive pairwise comparisons from the results matrix: for each entry where two tools both have predictions, the tool with the higher correctness score ``wins.''
We estimate parameters using the Iterative Luce Spectral Ranking (ILSR) algorithm~\citep{maystre2015} with $L_2$ regularization ($\alpha = 0.01$) via the \texttt{choix} library.
The estimated parameters are normalized to sum to 1, yielding a probability-like ranking score.

This approach handles the key challenge of incomplete data: tools evaluated on different subsets of entries can still be compared through their shared pairwise comparisons, weighted by the structure of the Plackett-Luce likelihood.

For \textsc{Hallmark}, this allows us to rank HaRC (20/1,063 entries) and verify-citations (71/1,063 entries) alongside complete-coverage baselines by leveraging the pairwise comparisons available on the entries each tool did evaluate.

\section{Pre-screening layer specification}
\label{app:prescreening}

The pre-screening layer implements three checks that run before external tool invocation:

\begin{enumerate}
    \item \textbf{DOI format validation}: Checks that DOI strings match the expected format (\texttt{10.XXXX/...}) and that the DOI prefix corresponds to a known registrant.
    \item \textbf{Year bounds checking}: Flags entries with publication years in the future or before 1900.
    \item \textbf{Author name heuristics}: Detects common placeholder patterns (``John Doe,'' ``A.\ Author,'' single-word author names, repeated names).
\end{enumerate}

Pre-screening results are tagged with \texttt{[Pre-screening override]} in the reason string to maintain transparency about which detections come from the pre-screening layer vs.\ the external tool.

\section{Baseline implementation details}
\label{app:baselines}

All baselines are implemented as Python wrappers conforming to the \textsc{Hallmark} baseline interface.
Each wrapper:
(1)~converts \textsc{Hallmark} \texttt{BenchmarkEntry} objects to the tool's expected input format,
(2)~invokes the tool,
(3)~maps the tool's output to a \textsc{Hallmark} \texttt{Prediction} with label, confidence, and reason.

\paragraph{Timeout handling.}
Each baseline is subject to a per-entry timeout (default: 60 seconds).
Entries that timeout are assigned a default prediction of \texttt{VALID} with confidence 0.5, following the conservative assumption that unverifiable entries should not be flagged.

\paragraph{Rate limiting.}
HaRC and verify-citations are subject to Semantic Scholar and Google Scholar rate limits.
For reproducibility, we provide pre-computed reference results in \texttt{data/v1.0/baseline\_results/}, generated by running the tools locally without rate-limit constraints.
CI validates these reference results by checksum rather than re-running the tools.

\section{Temporal analysis}
\label{app:temporal}

\paragraph{Baseline temporal consistency.}
We assign entries to three temporal segments: pre-2023 (papers published before January 2023), 2023--2024, and 2025+.
For bibtex-updater, detection rates are consistent across segments ($\pm$2\%), indicating robust performance across publication periods.
DOI-only shows a slight improvement on newer entries, likely because recent papers more consistently include DOIs.
Tools relying on static databases may degrade on newer entries as coverage lags, while live-API tools maintain consistent performance.

\paragraph{GPT-5.1 temporal probe.}
To test whether LLM-based verifiers degrade on papers beyond their training cutoff, we construct a 60-entry temporal probe set: 30~valid entries scraped from arXiv (2026 submissions) and DBLP (2024--2025 conference proceedings), plus 30~hallucinated variants generated across all three difficulty tiers using the standard perturbation pipeline.
We then evaluate GPT-5.1 on this probe and compare against its performance on the full benchmark (2021--2023 valid entries).

\cref{tab:temporal_probe} shows the results.
The false positive rate increases $3.7{\times}$---from 17.1\% on the baseline dataset to 63.3\% on recent papers---while the detection rate inflates from 79.7\% to 96.7\%.
This pattern is consistent with a model that defaults to \texttt{HALLUCINATED} when it cannot confirm a reference from parametric memory: it catches nearly all actual hallucinations but simultaneously flags 19 of 30 genuine recent papers.
ECE doubles from 0.107 to 0.238, indicating that the model's confidence scores become poorly calibrated on out-of-distribution temporal inputs.

Importantly, the degradation is asymmetric across tiers.
Tier~1 (fabricated DOIs, nonexistent venues) and Tier~2 (wrong venues, swapped authors) hallucinations are detected at 100\%, matching or exceeding baseline performance---these types contain structural anomalies that do not require temporal knowledge.
Tier~3 detection drops only slightly (90\% vs.\ baseline), suggesting that subtle perturbations remain detectable even for recent papers.
The precision collapse is driven entirely by false positives on \emph{valid} entries, not by missed hallucinations.

\begin{table}[h]
\centering
\caption{GPT-5.1 temporal robustness: full-dataset baseline (2021--2023 valid entries) vs.\ temporal probe (2024--2026 valid entries). The probe reveals a $3.7{\times}$ FPR increase driven by the model's inability to confirm recent publications.}
\label{tab:temporal_probe}
\small
\begin{tabular}{lcc}
\toprule
\textbf{Metric} & \textbf{Baseline} & \textbf{Temporal probe} \\
\midrule
Detection Rate      & 79.7\% & 96.7\% \\
False Positive Rate & 17.1\% & 63.3\% \\
F1 (Hallucination)  & 0.822  & 0.744  \\
ECE                 & 0.107  & 0.238  \\
\midrule
Tier~1 DR           & 93.3\% & 100\%  \\
Tier~2 DR           & 78.8\% & 100\%  \\
Tier~3 DR           & 63.8\% & 90.0\% \\
\bottomrule
\end{tabular}
\end{table}

These findings have two implications.
First, \textbf{temporal coverage matters}: benchmarks using only older valid entries will overestimate LLM-based verifier precision, because the model can confirm those papers from training data.
Second, \textbf{the failure mode is predictable}: LLMs treat unfamiliar papers as suspicious, making false positive rate the key metric to monitor as training cutoffs shift.
The generation pipeline now supports adaptive temporal scraping (\texttt{scrape\_arxiv\_recent()}) to keep the benchmark's valid entries current with the research literature.
\cref{fig:temporal} visualizes the temporal degradation pattern.

\begin{figure}[h]
\centering
\includegraphics[width=0.65\linewidth]{figures/temporal_robustness.pdf}
\caption{GPT-5.1 temporal robustness: detection rate and false positive rate on 2021--2023 baseline entries vs.\ 2024--2026 temporal probe entries. The $3.7{\times}$ FPR increase reflects the model's tendency to flag unfamiliar recent papers as hallucinated.}
\label{fig:temporal}
\end{figure}

\section{Infrastructure documentation}
\label{app:infrastructure}

\textsc{Hallmark} is distributed as a pip-installable Python package with the following components:

\begin{itemize}
    \item \textbf{CLI}: \texttt{hallmark evaluate}, \texttt{hallmark stats}, \texttt{hallmark leaderboard}, \texttt{hallmark contribute}
    \item \textbf{Python API}: \texttt{hallmark.dataset.loader.load\_split()}, \texttt{hallmark.evaluation.metrics.evaluate()}, \texttt{hallmark.evaluation.ranking.rank\_tools()}
    \item \textbf{Baseline registry}: \texttt{hallmark.baselines.registry.\{list\_baselines, check\_available, run\_baseline\}}
    \item \textbf{CI workflows}: \texttt{tests.yml} (test suite across Python 3.10--3.13), \texttt{baselines.yml} (weekly baseline evaluation)
\end{itemize}

\paragraph{Installation.}
\begin{verbatim}
pip install hallmark                    # Core
pip install hallmark[baselines]         # With baseline dependencies
pip install hallmark[ranking]           # With Plackett-Luce support
pip install hallmark[all]               # Everything
\end{verbatim}

\section{Datasheet for \textsc{Hallmark}}
\label{app:datasheet}

Following \citet{gebru2021datasheets}, we provide a datasheet for the \textsc{Hallmark} dataset.

\paragraph{Motivation.}
\textsc{Hallmark} was created to provide a standardized benchmark for evaluating citation hallucination detection tools, motivated by the NeurIPS 2025 incident and subsequent audits.

\paragraph{Composition.}
The dataset contains 2,554 BibTeX entries: 980 valid entries scraped from DBLP and arXiv and 1,574 hallucinated entries generated through perturbation, LLM generation, adversarial crafting, and real-world collection across 14 hallucination types.
Each entry includes 6 binary sub-test labels.

\paragraph{Collection process.}
Valid entries were scraped from the DBLP API and verified against CrossRef and Semantic Scholar.
Hallucinated entries were generated using the methods described in \cref{sec:dataset} and \cref{app:dataset}.

\paragraph{Preprocessing.}
BibTeX records were normalized to a consistent field ordering.
Unicode characters were preserved.
Entries were split into dev/test/hidden sets with stratified sampling across hallucination types and tiers.

\paragraph{Uses.}
\textsc{Hallmark} is intended for evaluating and comparing citation verification tools.
It should not be used to train hallucination generators or to generate convincing fake citations.

\paragraph{Distribution.}
The dataset is distributed under the MIT license via GitHub and PyPI.
The hidden test set is not publicly distributed.

\paragraph{Maintenance.}
The benchmark is maintained by the authors and accepts community contributions through a structured submission process (pull requests to the GitHub repository, validated by automated checks for BibTeX well-formedness, sub-test consistency, and duplicate detection).

\section{Co-designed tool evaluation}
\label{app:codesign}

\texttt{bibtex-updater}~\citep{bibtexupdater} is a multi-database cross-referencing tool developed by the authors of this benchmark.
We report its results separately to maintain transparency about the co-design relationship: the benchmark taxonomy and sub-test structure were informed by the tool's detection capabilities, creating a potential circularity that could inflate its apparent performance.

\begin{table}[h]
\caption{Results for \texttt{bibtex-updater} (co-designed by the authors). Reported separately from independent tools in \cref{tab:results} due to co-design bias concerns.}
\label{tab:codesign}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Tool} & \textbf{DR $\uparrow$} & \textbf{FPR $\downarrow$} & \textbf{F1 $\uparrow$} & \textbf{TW-F1 $\uparrow$} & \textbf{ECE $\downarrow$} & \textbf{Cov.} \\
\midrule
bibtex-updater & 0.637 & 0.051 & 0.759 & 0.753 & 0.213 & 1.00 \\
Ensemble (doi+btx)$^\dagger$ & 0.208 & 0.007 & 0.342 & 0.296 & 0.289 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\noindent $^\dagger$The ensemble uses confidence-weighted voting: an entry is flagged as hallucinated only when the weighted vote exceeds a threshold.
Because DOI-only assigns moderate confidence (0.5--0.7) while bibtex-updater assigns high confidence to both hallucinated and valid predictions, the ensemble's threshold filters out many DOI-only detections, producing lower DR than either tool alone.
This conservative behavior reduces false positives (FPR = 0.007) at the cost of recall.

\paragraph{Per-type breakdown for bibtex-updater.}
\cref{tab:codesign_pertype} shows per-type detection rates for bibtex-updater.
The tool achieves perfect detection on \texttt{future\_date} (1.000) and strong performance on \texttt{placeholder\_authors} (0.853) and \texttt{preprint\_as\_published} (0.742), but struggles with subtle metadata perturbations like \texttt{swapped\_authors} (0.412) and \texttt{near\_miss\_title} (0.486).

\begin{table}[h]
\caption{Per-type detection rates for \texttt{bibtex-updater} on \texttt{dev\_public}.}
\label{tab:codesign_pertype}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llc}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{bibtex-updater} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi}        & 0.590 \\
& \texttt{nonexistent\_venue}     & 0.719 \\
& \texttt{placeholder\_authors}   & 0.853 \\
& \texttt{future\_date}           & 1.000 \\
\midrule
\multirow{5}{*}{2}
& \texttt{chimeric\_title}        & 0.629 \\
& \texttt{wrong\_venue}           & 0.714 \\
& \texttt{partial\_author\_list}  & 0.700 \\
& \texttt{preprint\_as\_pub.}     & 0.742 \\
& \texttt{hybrid\_fabrication}    & 0.543 \\
\midrule
\multirow{2}{*}{3}
& \texttt{near\_miss\_title}      & 0.486 \\
& \texttt{plausible\_fabrication} & 0.655 \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\scriptsize Stress}}
& \texttt{merged\_citation}       & 0.633 \\
& \texttt{arxiv\_version\_mismatch} & 0.567 \\
& \texttt{swapped\_authors}       & 0.412 \\
\bottomrule
\end{tabular}
\end{table}

\noindent While \texttt{bibtex-updater} achieves a detection rate of 0.637 and F1 of 0.759, readers should interpret these results with caution:
(1)~the tool's multi-database verification strategy was developed concurrently with the benchmark design;
(2)~the sub-test structure (DOI resolution, title existence, author matching, venue verification) mirrors the tool's internal verification pipeline;
(3)~the pre-screening layer was designed to complement the tool's known blind spots.

Despite these concerns, \texttt{bibtex-updater} serves a useful diagnostic role: it demonstrates the performance gap between API-based cross-referencing tools (DR 0.637) and LLM-based verifiers (GPT-5.1: DR 0.797), while achieving better calibration (ECE 0.213 vs.\ GPT-5.1's 0.107) and lower false positive rate (FPR 0.051 vs.\ 0.171).
We encourage independent tool developers to submit evaluations via \textsc{Hallmark}'s contribution framework to establish unbiased baselines.

\section{Additional figures}
\label{app:figures}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/overall_comparison.pdf}
\caption{Overall comparison of baseline performance across primary metrics.}
\label{fig:overall}
\end{figure}

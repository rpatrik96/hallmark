\section{Full taxonomy details}
\label{app:taxonomy}

\cref{tab:taxonomy_full} provides the complete taxonomy with BibTeX examples for each hallucination type.

\begin{table}[h]
\caption{Full taxonomy with example BibTeX snippets illustrating each hallucination type. Red text indicates the hallucinated field.}
\label{tab:taxonomy_full}
\centering
\small
\begin{tabular}{clp{8cm}}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{Example (hallucinated field in red)} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi} & \texttt{doi = \{\textcolor{red}{10.9999/nips2024.1847}\}} \\
& \texttt{nonexistent\_venue} & \texttt{booktitle = \{\textcolor{red}{Intl.\ Conf.\ on Advanced AI Systems}\}} \\
& \texttt{placeholder\_authors} & \texttt{author = \{\textcolor{red}{John Doe and Jane Smith}\}} \\
& \texttt{future\_date} & \texttt{year = \{\textcolor{red}{2030}\}} \\
\midrule
\multirow{7}{*}{2}
& \texttt{chimeric\_title} & Real authors, \texttt{title = \{\textcolor{red}{A Novel Approach...}\}} (nonexistent) \\
& \texttt{wrong\_venue} & Real paper, \texttt{booktitle = \{\textcolor{red}{ICML}\}} (actually NeurIPS) \\
& \texttt{author\_mismatch} & Real title, \texttt{author = \{\textcolor{red}{Wrong Author List}\}} \\
& \texttt{preprint\_as\_published} & arXiv paper, \texttt{booktitle = \{\textcolor{red}{NeurIPS}\}} (never published) \\
& \texttt{hybrid\_fabrication} & Valid DOI resolves, but \texttt{title = \{\textcolor{red}{...}\}} doesn't match \\
& \texttt{merged\_citation} & Authors from paper A, \texttt{title} from \textcolor{red}{paper B}, venue from C \\
& \texttt{partial\_author\_list} & Real paper, \texttt{author = \{\textcolor{red}{First and Last}\}} (middle dropped) \\
\midrule
\multirow{3}{*}{3}
& \texttt{near\_miss\_title} & \texttt{title = \{Attention Is All You \textcolor{red}{Want}\}} (vs.\ ``Need'') \\
& \texttt{plausible\_fabrication} & Entirely fabricated, all fields realistic but nonexistent \\
& \texttt{version\_confusion} & Claims from \textcolor{red}{v1} that were corrected in v2 \\
\bottomrule
\end{tabular}
\end{table}

\section{Dataset construction details}
\label{app:dataset}

\paragraph{DBLP scraping.}
Valid entries were scraped from the DBLP API (\texttt{dblp.org/search/publ/api}) using venue-specific queries for NeurIPS, ICML, ICLR, AAAI, and CVPR.
We retrieved BibTeX records, verified DOI resolution via CrossRef, and confirmed title existence in Semantic Scholar.
Entries failing any verification step were excluded.

\paragraph{Perturbation pipeline.}
Systematic perturbations follow deterministic rules per hallucination type:
\begin{itemize}
    \item \texttt{fabricated\_doi}: Replace DOI with a non-resolving DOI using one of 20 non-existent prefixes and four suffix styles (path, identifier, year-indexed, and conference-indexed) to avoid template-detectable patterns.
    \item \texttt{nonexistent\_venue}: Replace venue with an LLM-generated plausible but nonexistent conference name.
    \item \texttt{placeholder\_authors}: Replace author list with common placeholder names.
    \item \texttt{future\_date}: Set year to current year + 5.
    \item \texttt{chimeric\_title}: Keep authors from paper A, replace title with LLM-generated plausible title.
    \item \texttt{wrong\_venue}: Keep all fields but swap venue with a different real venue.
    \item \texttt{author\_mismatch}: Keep title and venue, replace authors with those from a different paper.
    \item \texttt{preprint\_as\_published}: Take an arXiv-only paper and add a fabricated venue field.
    \item \texttt{hybrid\_fabrication}: Keep a valid DOI but replace title and authors with fabricated metadata.
    \item \texttt{near\_miss\_title}: Modify 1--2 words via six strategies: synonym substitution (POS-safe pairs), plural/singular flipping, British/American spelling swap, abbreviation expansion/contraction (e.g., ``RL'' $\leftrightarrow$ ``Reinforcement Learning''), hyphenation toggling (e.g., ``self-supervised'' $\leftrightarrow$ ``self supervised''), and article removal.
    \item \texttt{plausible\_fabrication}: LLM-generate a complete, realistic but nonexistent entry.
    \item \texttt{merged\_citation}: Combine metadata from 2--3 real papers into one entry (e.g., authors from paper A, title from paper B, venue from paper C).
    \item \texttt{partial\_author\_list}: Take a real paper and drop one or more middle co-authors, keeping only the first and last.
    \item \texttt{version\_confusion}: Cite specific claims from superseded arXiv versions.
\end{itemize}

\paragraph{LLM-generated entries.}
We prompted GPT-5.1 to generate plausible but fictional citations for types requiring coherent fabrication (\texttt{plausible\_fabrication}, \texttt{chimeric\_title}, \texttt{fabricated\_doi}).
Each prompt requested a BibTeX entry with realistic metadata for a specified ML venue and year range.
Generated entries were verified against CrossRef and DBLP: entries with title similarity $\geq 85\%$ (token-sort ratio) and author Jaccard similarity $\geq 0.5$ to any real paper were flagged as potential duplicates of real work and excluded.
This filtering removed approximately 12\% of generated entries, documenting an LLM recall failure rate where the model reproduces real papers rather than fabricating new ones.
The remaining entries were assigned sub-test labels based on their hallucination type's expected failure pattern.

\paragraph{Real-world collection.}
We harvested 72 hallucinated citations from three documented incident studies: GPTZero's NeurIPS 2025 analysis, GhostCite~\citep{ghostcite2026}, and HalluCitation~\citep{hallucitation2026}.
Each entry was mapped to the closest taxonomy type based on its failure mode (e.g., a citation with a non-resolving DOI was classified as \texttt{fabricated\_doi}; a citation to a non-existent paper with plausible metadata was classified as \texttt{plausible\_fabrication}).
The real-world sample is type-skewed: 55\% are \texttt{plausible\_fabrication}, reflecting the predominant LLM failure mode in practice.
Types without real-world examples (\texttt{merged\_citation}, \texttt{partial\_author\_list}, \texttt{version\_confusion}) are flagged as theoretically motivated rather than empirically observed.

\paragraph{Adversarial crafting.}
We manually constructed entries designed to evade specific detection strategies.
These include entries with DOIs that resolve to unrelated papers (\texttt{hybrid\_fabrication}), entries combining metadata from multiple real papers (\texttt{merged\_citation}), and entries with plausible but non-existent venues chosen to be close to real venue names.
Adversarial entries stress-test tool robustness beyond what template-based perturbation achieves.

\paragraph{Quality control.}
Every generated entry passes through automated validation:
(1)~BibTeX well-formedness check (all required fields present, valid syntax),
(2)~sub-test label consistency (sub-test ground truth matches the hallucination type's expected failure pattern),
(3)~cross-validation with the valid entry pool to prevent accidental duplicates.

\section{Core subset analysis}
\label{app:core-subset}

To verify that scaling the hallucinated entries (from 134 to 975) does not distort evaluation signals, we re-evaluate all baselines on the original 521-entry core subset (71 hallucinated in dev, 43 in test).
Aggregate metrics differ by less than 2\% between the core subset and the full dataset across all baselines: detection rate changes by at most 0.015, F1 by at most 0.02, and tier-weighted F1 by at most 0.018.
Per-type rankings are preserved---no baseline changes rank on any type.
We conclude that the scaled entries are consistent with the original distribution and do not introduce systematic bias.

\section{Full per-type results}
\label{app:pertype}

\cref{tab:pertype_full} reports detection rate, F1, and count for every hallucination type and baseline.

\begin{table}[h]
\caption{Per-type detection rate on \texttt{dev\_public} for all baselines. All types have 30 instances.}
\label{tab:pertype_full}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llccccc}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{btx-upd} & \textbf{Ensemble} & \textbf{HaRC$^*$} & \textbf{DOI} & \textbf{v-cit$^*$} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi}        & 1.000 & --- & --- & 1.000 & --- \\
& \texttt{nonexistent\_venue}     & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{placeholder\_authors}   & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{future\_date}           & 1.000 & --- & --- & 0.000 & --- \\
\midrule
\multirow{7}{*}{2}
& \texttt{chimeric\_title}        & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{wrong\_venue}           & 0.633 & --- & --- & 0.000 & --- \\
& \texttt{author\_mismatch}       & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{preprint\_as\_pub.}     & 0.533 & --- & --- & 0.000 & --- \\
& \texttt{hybrid\_fabrication}    & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{merged\_citation}       & \multicolumn{5}{c}{\emph{evaluation pending}} \\
& \texttt{partial\_author\_list}  & \multicolumn{5}{c}{\emph{evaluation pending}} \\
\midrule
\multirow{3}{*}{3}
& \texttt{near\_miss\_title}      & 1.000 & --- & --- & 0.000 & --- \\
& \texttt{plausible\_fabrication} & 1.000 & --- & --- & 1.000 & --- \\
& \texttt{version\_confusion}     & 1.000 & --- & --- & 0.000 & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent All hallucination types have $\geq$30 instances per public split in the expanded dataset.
LLM-generated entries concentrate on \texttt{plausible\_fabrication} (40\%) and \texttt{fabricated\_doi} (33\%), reflecting that GPT-5.1 predominantly produces citations that cannot be verified in any database rather than entries with partially correct metadata.
Entries marked --- indicate that per-type breakdowns are not available for the ensemble and rate-limited baselines at this granularity.

\section{Statistical analysis}
\label{app:statistics}

\paragraph{Bootstrap confidence intervals.}
All aggregate metrics in \cref{tab:results} are accompanied by 95\% confidence intervals computed via stratified bootstrap with 10{,}000 resamples.
Stratification by hallucination type ensures that the resampled datasets preserve the original type distribution, preventing bootstrap bias from underrepresented types.

\paragraph{Significance testing.}
We use paired bootstrap tests~\citep{efron1994bootstrap} to compare tools pairwise.
For each pair of tools $(A, B)$, we resample entries with replacement and compute $\Delta = \text{F1}_A - \text{F1}_B$ for each resample.
The $p$-value is the fraction of resamples where $\Delta \leq 0$.

\paragraph{Per-type power analysis.}
With $n = 30$ instances per type per split, binary classification metrics have substantially narrower confidence intervals than the original 10-instance design.
For detection rate, a type with 30 hallucinated entries and 90\% true detection rate has a 95\% Clopper-Pearson confidence interval of $[0.74, 0.98]$, compared to $[0.55, 0.998]$ for $n=10$.
We report per-type results with improved statistical confidence, enabling more definitive type-level claims.
As the dataset grows through community contributions, per-type statistical power will continue to improve.

\paragraph{Tier weight sensitivity.}
We evaluate tier-weighted F1 under five weighting schemes: uniform $\{1,1,1\}$ (equivalent to standard macro-F1 across tiers), linear $\{1,2,3\}$ (default), quadratic $\{1,4,9\}$, log $\{\log(2), \log(3), \log(4)\}$, and inverse difficulty (weights proportional to $1 - \text{mean DR}$ across tools).
Tool rankings are stable across all schemes: bibtex-updater achieves TW-F1 of 0.962 (uniform), 0.969 (linear), 0.977 (quadratic), 0.966 (log), and 0.961 (inverse difficulty).
DOI-only achieves 0.312, 0.294, 0.280, 0.300, and 0.297 under the same schemes.
The relative ordering of tools is preserved under all weighting schemes, confirming that our conclusions are robust to the specific choice of tier weights.
The narrow sensitivity range for bibtex-updater (0.961--0.977) reflects its uniformly high per-tier performance rather than inherent metric robustness; tools with greater tier-differential accuracy would show more variation, making this analysis more informative as the baseline ecosystem matures.

\section{Plackett-Luce ranking}
\label{app:ranking}

The Plackett-Luce model~\citep{plackett1975,luce1959} assigns a positive strength parameter $\theta_j$ to each tool $j \in \{1, \ldots, J\}$.
Given a ranking $\sigma$ over a subset $S$ of tools, the probability of observing $\sigma$ is:
\begin{equation}
P(\sigma \mid \boldsymbol{\theta}) = \prod_{k=1}^{|S|} \frac{\theta_{\sigma(k)}}{\sum_{l=k}^{|S|} \theta_{\sigma(l)}}.
\end{equation}

In \textsc{Hallmark}, we derive pairwise comparisons from the results matrix: for each entry where two tools both have predictions, the tool with the higher correctness score ``wins.''
We estimate parameters using the Iterative Luce Spectral Ranking (ILSR) algorithm~\citep{maystre2015} with $L_2$ regularization ($\alpha = 0.01$) via the \texttt{choix} library.
The estimated parameters are normalized to sum to 1, yielding a probability-like ranking score.

This approach handles the key challenge of incomplete data: tools evaluated on different subsets of entries can still be compared through their shared pairwise comparisons, weighted by the structure of the Plackett-Luce likelihood.

For \textsc{Hallmark}, this allows us to rank HaRC (20/840 entries) and verify-citations (71/840 entries) alongside complete-coverage baselines by leveraging the pairwise comparisons available on the entries each tool did evaluate.

\section{Pre-screening layer specification}
\label{app:prescreening}

The pre-screening layer implements three checks that run before external tool invocation:

\begin{enumerate}
    \item \textbf{DOI format validation}: Checks that DOI strings match the expected format (\texttt{10.XXXX/...}) and that the DOI prefix corresponds to a known registrant.
    \item \textbf{Year bounds checking}: Flags entries with publication years in the future or before 1900.
    \item \textbf{Author name heuristics}: Detects common placeholder patterns (``John Doe,'' ``A.\ Author,'' single-word author names, repeated names).
\end{enumerate}

Pre-screening results are tagged with \texttt{[Pre-screening override]} in the reason string to maintain transparency about which detections come from the pre-screening layer vs.\ the external tool.

\section{Baseline implementation details}
\label{app:baselines}

All baselines are implemented as Python wrappers conforming to the \textsc{Hallmark} baseline interface.
Each wrapper:
(1)~converts \textsc{Hallmark} \texttt{BenchmarkEntry} objects to the tool's expected input format,
(2)~invokes the tool,
(3)~maps the tool's output to a \textsc{Hallmark} \texttt{Prediction} with label, confidence, and reason.

\paragraph{Timeout handling.}
Each baseline is subject to a per-entry timeout (default: 60 seconds).
Entries that timeout are assigned a default prediction of \texttt{VALID} with confidence 0.5, following the conservative assumption that unverifiable entries should not be flagged.

\paragraph{Rate limiting.}
HaRC and verify-citations are subject to Semantic Scholar and Google Scholar rate limits.
For reproducibility, we provide pre-computed reference results in \texttt{data/v1.0/baseline\_results/}, generated by running the tools locally without rate-limit constraints.
CI validates these reference results by checksum rather than re-running the tools.

\section{Temporal analysis}
\label{app:temporal}

We assign entries to three temporal segments: pre-2023 (papers published before January 2023), 2023--2024, and 2025+.
For bibtex-updater, detection rates are consistent across segments ($\pm$2\%), indicating robust performance across publication periods.
DOI-only shows a slight improvement on newer entries, likely because recent papers more consistently include DOIs.
Tools relying on static databases may degrade on newer entries as coverage lags, while live-API tools maintain consistent performance.

\section{Infrastructure documentation}
\label{app:infrastructure}

\textsc{Hallmark} is distributed as a pip-installable Python package with the following components:

\begin{itemize}
    \item \textbf{CLI}: \texttt{hallmark evaluate}, \texttt{hallmark stats}, \texttt{hallmark leaderboard}, \texttt{hallmark contribute}
    \item \textbf{Python API}: \texttt{hallmark.dataset.loader.load\_split()}, \texttt{hallmark.evaluation.metrics.evaluate()}, \texttt{hallmark.evaluation.ranking.rank\_tools()}
    \item \textbf{Baseline registry}: \texttt{hallmark.baselines.registry.\{list\_baselines, check\_available, run\_baseline\}}
    \item \textbf{CI workflows}: \texttt{tests.yml} (test suite across Python 3.10--3.13), \texttt{baselines.yml} (weekly baseline evaluation)
\end{itemize}

\paragraph{Installation.}
\begin{verbatim}
pip install hallmark                    # Core
pip install hallmark[baselines]         # With baseline dependencies
pip install hallmark[ranking]           # With Plackett-Luce support
pip install hallmark[all]               # Everything
\end{verbatim}

\section{Datasheet for \textsc{Hallmark}}
\label{app:datasheet}

Following \citet{gebru2021datasheets}, we provide a datasheet for the \textsc{Hallmark} dataset.

\paragraph{Motivation.}
\textsc{Hallmark} was created to provide a standardized benchmark for evaluating citation hallucination detection tools, motivated by the NeurIPS 2025 incident and subsequent audits.

\paragraph{Composition.}
The dataset contains 2,184 BibTeX entries: 918 valid entries scraped from DBLP and 1,266 hallucinated entries generated through perturbation, LLM generation, adversarial crafting, and real-world collection across 14 hallucination types.
Each entry includes 6 binary sub-test labels.

\paragraph{Collection process.}
Valid entries were scraped from the DBLP API and verified against CrossRef and Semantic Scholar.
Hallucinated entries were generated using the methods described in \cref{sec:dataset} and \cref{app:dataset}.

\paragraph{Preprocessing.}
BibTeX records were normalized to a consistent field ordering.
Unicode characters were preserved.
Entries were split into dev/test/hidden sets with stratified sampling across hallucination types and tiers.

\paragraph{Uses.}
\textsc{Hallmark} is intended for evaluating and comparing citation verification tools.
It should not be used to train hallucination generators or to generate convincing fake citations.

\paragraph{Distribution.}
The dataset is distributed under the MIT license via GitHub and PyPI.
The hidden test set is not publicly distributed.

\paragraph{Maintenance.}
The benchmark is maintained by the authors and accepts community contributions through the structured submission process described in \cref{sec:contribution}.

\section{Additional figures}
\label{app:figures}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/overall_comparison.pdf}
\caption{Overall comparison of baseline performance across primary metrics.}
\label{fig:overall}
\end{figure}

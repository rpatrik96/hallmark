\section{Full taxonomy details}
\label{app:taxonomy}

\cref{tab:taxonomy_full} provides the complete taxonomy with BibTeX examples for each hallucination type.

\begin{table}[h]
\caption{Full taxonomy with example BibTeX snippets illustrating each hallucination type. Red text indicates the hallucinated field.}
\label{tab:taxonomy_full}
\centering
\small
\begin{tabular}{clp{8cm}}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{Example (hallucinated field in red)} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi} & \texttt{doi = \{\textcolor{red}{10.9999/nips2024.1847}\}} \\
& \texttt{nonexistent\_venue} & \texttt{booktitle = \{\textcolor{red}{Intl.\ Conf.\ on Advanced AI Systems}\}} \\
& \texttt{placeholder\_authors} & \texttt{author = \{\textcolor{red}{John Doe and Jane Smith}\}} \\
& \texttt{future\_date} & \texttt{year = \{\textcolor{red}{2030}\}} \\
\midrule
\multirow{5}{*}{2}
& \texttt{chimeric\_title} & Real authors, \texttt{title = \{\textcolor{red}{A Novel Approach...}\}} (nonexistent) \\
& \texttt{wrong\_venue} & Real paper, \texttt{booktitle = \{\textcolor{red}{ICML}\}} (actually NeurIPS) \\
& \texttt{author\_mismatch} & Real title, \texttt{author = \{\textcolor{red}{Wrong Author List}\}} \\
& \texttt{preprint\_as\_published} & arXiv paper, \texttt{booktitle = \{\textcolor{red}{NeurIPS}\}} (never published) \\
& \texttt{hybrid\_fabrication} & Valid DOI resolves, but \texttt{title = \{\textcolor{red}{...}\}} doesn't match \\
\midrule
\multirow{2}{*}{3}
& \texttt{near\_miss\_title} & \texttt{title = \{Attention Is All You \textcolor{red}{Want}\}} (vs.\ ``Need'') \\
& \texttt{plausible\_fabrication} & Entirely fabricated, all fields realistic but nonexistent \\
\midrule
\multirow{3}{*}{\rotatebox{90}{\scriptsize Stress}}
& \texttt{merged\_citation} & Authors from paper A, \texttt{title} from \textcolor{red}{paper B}, venue from C \\
& \texttt{partial\_author\_list} & Real paper, \texttt{author = \{\textcolor{red}{First and Last}\}} (middle dropped) \\
& \texttt{arxiv\_version\_mismatch} & arXiv preprint cited with \textcolor{red}{wrong venue} and shifted year \\
\bottomrule
\end{tabular}
\end{table}

\section{Real-world incident mapping}
\label{app:real-world-mapping}

We mapped 72 real-world hallucinated citations from three documented incident studies to our taxonomy.
\cref{tab:realworld} shows the mapping results.

\begin{table}[h]
\caption{Mapping of 72 real-world hallucinated citations to \textsc{Hallmark} taxonomy types. Citations were sourced from GPTZero's NeurIPS 2025 analysis, GhostCite, and HalluCitation.}
\label{tab:realworld}
\centering
\small
\begin{tabular}{llr}
\toprule
\textbf{Taxonomy type} & \textbf{Tier} & \textbf{Count} \\
\midrule
\texttt{plausible\_fabrication} & 3 & 40 \\
\texttt{fabricated\_doi} & 1 & 10 \\
\texttt{chimeric\_title} & 2 & 8 \\
\texttt{near\_miss\_title} & 3 & 5 \\
\texttt{wrong\_venue} & 2 & 4 \\
\texttt{author\_mismatch} & 2 & 3 \\
\texttt{hybrid\_fabrication} & 2 & 2 \\
\midrule
\textbf{Total mapped} & & \textbf{72} \\
\bottomrule
\end{tabular}
\end{table}

\noindent Of the 72 real-world citations, 57 (79\%) map directly to existing taxonomy types.
The remaining 15 exhibit compound failure modes (e.g., fabricated DOI \emph{and} wrong venue simultaneously); we assigned these to the highest-tier applicable type.
The dominance of \texttt{plausible\_fabrication} (55\%) reflects the predominant LLM failure mode: models generate coherent but entirely fictional references rather than subtly corrupting real ones.
Three taxonomy types---\texttt{merged\_citation}, \texttt{partial\_author\_list}, and \texttt{arxiv\_version\_mismatch}---have zero real-world instances, motivating their designation as stress-test types (\cref{sec:taxonomy}).

\section{Dataset construction details}
\label{app:dataset}

\paragraph{DBLP scraping.}
Valid entries were scraped from the DBLP API (\texttt{dblp.org/search/publ/api}) using venue-specific queries for NeurIPS, ICML, ICLR, AAAI, and CVPR.
We retrieved BibTeX records, verified DOI resolution via CrossRef, and confirmed title existence in Semantic Scholar.
Entries failing any verification step were excluded.

\paragraph{Perturbation pipeline.}
Systematic perturbations follow deterministic rules per hallucination type:
\begin{itemize}
    \item \texttt{fabricated\_doi}: Replace DOI with a non-resolving DOI using one of 20 non-existent prefixes and four suffix styles (path, identifier, year-indexed, and conference-indexed) to avoid template-detectable patterns.
    \item \texttt{nonexistent\_venue}: Replace venue with an LLM-generated plausible but nonexistent conference name.
    \item \texttt{placeholder\_authors}: Replace author list with common placeholder names.
    \item \texttt{future\_date}: Set year to current year + 5.
    \item \texttt{chimeric\_title}: Keep authors from paper A, replace title with LLM-generated plausible title.
    \item \texttt{wrong\_venue}: Keep all fields but swap venue with a different real venue.
    \item \texttt{author\_mismatch}: Keep title and venue, replace authors with those from a different paper.
    \item \texttt{preprint\_as\_published}: Take an arXiv-only paper and add a fabricated venue field.
    \item \texttt{hybrid\_fabrication}: Keep a valid DOI but replace title and authors with fabricated metadata.
    \item \texttt{near\_miss\_title}: Modify 1--2 words via six strategies: synonym substitution (POS-safe pairs), plural/singular flipping, British/American spelling swap, abbreviation expansion/contraction (e.g., ``RL'' $\leftrightarrow$ ``Reinforcement Learning''), hyphenation toggling (e.g., ``self-supervised'' $\leftrightarrow$ ``self supervised''), and article removal.
    \item \texttt{plausible\_fabrication}: LLM-generate a complete, realistic but nonexistent entry.
    \item \texttt{merged\_citation}: Combine metadata from 2--3 real papers into one entry (e.g., authors from paper A, title from paper B, venue from paper C).
    \item \texttt{partial\_author\_list}: Take a real paper and drop one or more middle co-authors, keeping only the first and last.
    \item \texttt{arxiv\_version\_mismatch}: Cite specific claims from superseded arXiv versions.
\end{itemize}

\paragraph{LLM-generated entries.}
We prompted GPT-5.1 to generate plausible but fictional citations for types requiring coherent fabrication (\texttt{plausible\_fabrication}, \texttt{chimeric\_title}, \texttt{fabricated\_doi}).
Each prompt requested a BibTeX entry with realistic metadata for a specified ML venue and year range.
Generated entries were verified against CrossRef and DBLP: entries with title similarity $\geq 85\%$ (token-sort ratio) and author Jaccard similarity $\geq 0.5$ to any real paper were flagged as potential duplicates of real work and excluded.
This filtering removed approximately 12\% of generated entries, documenting an LLM recall failure rate where the model reproduces real papers rather than fabricating new ones.
The remaining entries were assigned sub-test labels based on their hallucination type's expected failure pattern.

\paragraph{Real-world collection.}
We harvested 72 hallucinated citations from three documented incident studies: GPTZero's NeurIPS 2025 analysis, GhostCite~\citep{ghostcite2026}, and HalluCitation~\citep{hallucitation2026}.
Each entry was mapped to the closest taxonomy type based on its failure mode (e.g., a citation with a non-resolving DOI was classified as \texttt{fabricated\_doi}; a citation to a non-existent paper with plausible metadata was classified as \texttt{plausible\_fabrication}).
The real-world sample is type-skewed: 55\% are \texttt{plausible\_fabrication}, reflecting the predominant LLM failure mode in practice.
Types without real-world examples (\texttt{merged\_citation}, \texttt{partial\_author\_list}, \texttt{arxiv\_version\_mismatch}) are relegated to a separate stress-test split, evaluated independently from the main taxonomy.

\paragraph{Adversarial crafting.}
We manually constructed entries designed to evade specific detection strategies.
These include entries with DOIs that resolve to unrelated papers (\texttt{hybrid\_fabrication}), entries combining metadata from multiple real papers (\texttt{merged\_citation}), and entries with plausible but non-existent venues chosen to be close to real venue names.
Adversarial entries stress-test tool robustness beyond what template-based perturbation achieves.

\paragraph{Quality control.}
Every generated entry passes through automated validation:
(1)~BibTeX well-formedness check (all required fields present, valid syntax),
(2)~sub-test label consistency (sub-test ground truth matches the hallucination type's expected failure pattern),
(3)~cross-validation with the valid entry pool to prevent accidental duplicates.

\section{Core subset analysis}
\label{app:core-subset}

The hallucinated entries were scaled from 114 (71 in dev, 43 in test) to 816 (453 in dev, 363 in test) by generating additional entries across all main hallucination types.
To verify that this scaling does not distort evaluation signals, we re-evaluate all baselines on the original core subset containing only the 114 seed hallucinations alongside the 720 valid entries.
Aggregate metrics differ by less than 2\% between the core subset and the full dataset across all baselines: detection rate changes by at most 0.015, F1 by at most 0.02, and tier-weighted F1 by at most 0.018.
Per-type rankings are preserved---no baseline changes rank on any type.
We conclude that the scaled entries are consistent with the original distribution and do not introduce systematic bias.

\section{Full per-type results}
\label{app:pertype}

\cref{tab:pertype_full} reports detection rate for every hallucination type and baseline.

\begin{table}[h]
\caption{Per-type detection rate on \texttt{dev\_public} for independent tools. All main types have $\geq$29 instances. Stress-test types are evaluated in the separate \texttt{stress\_test} split.}
\label{tab:pertype_full}
\centering
\small
\setlength{\tabcolsep}{3pt}
\begin{tabular}{llccc}
\toprule
\textbf{Tier} & \textbf{Type} & \textbf{DOI} & \textbf{HaRC$^*$} & \textbf{v-cit$^*$} \\
\midrule
\multirow{4}{*}{1}
& \texttt{fabricated\_doi}        & 1.000 & --- & --- \\
& \texttt{nonexistent\_venue}     & 0.000 & --- & --- \\
& \texttt{placeholder\_authors}   & 0.000 & --- & --- \\
& \texttt{future\_date}           & 0.000 & --- & --- \\
\midrule
\multirow{5}{*}{2}
& \texttt{chimeric\_title}        & 0.000 & --- & --- \\
& \texttt{wrong\_venue}           & 0.000 & --- & --- \\
& \texttt{author\_mismatch}       & 0.000 & --- & --- \\
& \texttt{preprint\_as\_pub.}     & 0.000 & --- & --- \\
& \texttt{hybrid\_fabrication}    & 0.000 & --- & --- \\
\midrule
\multirow{2}{*}{3}
& \texttt{near\_miss\_title}      & 0.000 & --- & --- \\
& \texttt{plausible\_fabrication} & 1.000 & --- & --- \\
\bottomrule
\end{tabular}
\end{table}

\noindent All main hallucination types have $\geq$29 instances per public split.
Entries marked --- indicate insufficient coverage for per-type breakdowns (HaRC: 2.2\%, verify-citations: 7.9\%).
DOI-only detects only types with fabricated or missing DOIs (\texttt{fabricated\_doi}, \texttt{plausible\_fabrication}), confirming that DOI resolution is necessary but far from sufficient.
Note that \texttt{plausible\_fabrication} (Tier~3) shows 100\% detection by DOI-only: these entries are entirely fabricated, so their DOIs do not resolve.
The Tier~3 classification reflects the \emph{difficulty of recognizing the entry as fake from metadata alone}---absent a DOI check, these entries are the hardest to detect because all other fields appear realistic.
This highlights why multi-strategy evaluation matters: a tool relying solely on DOI checks would appear to ``solve'' this type while missing all Tier~2 types that use valid DOIs.

\section{Statistical analysis}
\label{app:statistics}

\paragraph{Bootstrap confidence intervals.}
All aggregate metrics in \cref{tab:results} are accompanied by 95\% confidence intervals computed via stratified bootstrap with 10{,}000 resamples.
Stratification by hallucination type ensures that the resampled datasets preserve the original type distribution, preventing bootstrap bias from underrepresented types.

\paragraph{Significance testing.}
We use paired bootstrap tests~\citep{efron1994bootstrap} to compare tools pairwise.
For each pair of tools $(A, B)$, we resample entries with replacement and compute $\Delta = \text{F1}_A - \text{F1}_B$ for each resample.
The $p$-value is the fraction of resamples where $\Delta \leq 0$.

\paragraph{Per-type power analysis.}
With $n \geq 29$ instances per type per split, binary classification metrics have substantially narrower confidence intervals than the original 10-instance design.
For detection rate, a type with 29 hallucinated entries and 90\% true detection rate has a 95\% Clopper-Pearson confidence interval of $[0.73, 0.98]$, compared to $[0.55, 0.998]$ for $n=10$.
We report per-type results with improved statistical confidence, enabling more definitive type-level claims.
As the dataset grows through community contributions, per-type statistical power will continue to improve.

\paragraph{Tier weight sensitivity.}
We evaluate tier-weighted F1 under five weighting schemes: uniform $\{1,1,1\}$ (equivalent to standard macro-F1 across tiers), linear $\{1,2,3\}$ (default), quadratic $\{1,4,9\}$, log $\{\log(2), \log(3), \log(4)\}$, and inverse difficulty (weights proportional to $1 - \text{mean DR}$ across tools).
Tool rankings are stable across all schemes: bibtex-updater achieves TW-F1 of 0.962 (uniform), 0.969 (linear), 0.977 (quadratic), 0.966 (log), and 0.961 (inverse difficulty).
DOI-only achieves 0.312, 0.294, 0.280, 0.300, and 0.297 under the same schemes.
The relative ordering of tools is preserved under all weighting schemes, confirming that our conclusions are robust to the specific choice of tier weights.
The narrow sensitivity range for bibtex-updater (0.961--0.977) reflects its uniformly high per-tier performance rather than inherent metric robustness; tools with greater tier-differential accuracy would show more variation, making this analysis more informative as the baseline ecosystem matures.

\section{Plackett-Luce ranking}
\label{app:ranking}

The Plackett-Luce model~\citep{plackett1975,luce1959} assigns a positive strength parameter $\theta_j$ to each tool $j \in \{1, \ldots, J\}$.
Given a ranking $\sigma$ over a subset $S$ of tools, the probability of observing $\sigma$ is:
\begin{equation}
P(\sigma \mid \boldsymbol{\theta}) = \prod_{k=1}^{|S|} \frac{\theta_{\sigma(k)}}{\sum_{l=k}^{|S|} \theta_{\sigma(l)}}.
\end{equation}

In \textsc{Hallmark}, we derive pairwise comparisons from the results matrix: for each entry where two tools both have predictions, the tool with the higher correctness score ``wins.''
We estimate parameters using the Iterative Luce Spectral Ranking (ILSR) algorithm~\citep{maystre2015} with $L_2$ regularization ($\alpha = 0.01$) via the \texttt{choix} library.
The estimated parameters are normalized to sum to 1, yielding a probability-like ranking score.

This approach handles the key challenge of incomplete data: tools evaluated on different subsets of entries can still be compared through their shared pairwise comparisons, weighted by the structure of the Plackett-Luce likelihood.

For \textsc{Hallmark}, this allows us to rank HaRC (20/903 entries) and verify-citations (71/903 entries) alongside complete-coverage baselines by leveraging the pairwise comparisons available on the entries each tool did evaluate.

\section{Pre-screening layer specification}
\label{app:prescreening}

The pre-screening layer implements three checks that run before external tool invocation:

\begin{enumerate}
    \item \textbf{DOI format validation}: Checks that DOI strings match the expected format (\texttt{10.XXXX/...}) and that the DOI prefix corresponds to a known registrant.
    \item \textbf{Year bounds checking}: Flags entries with publication years in the future or before 1900.
    \item \textbf{Author name heuristics}: Detects common placeholder patterns (``John Doe,'' ``A.\ Author,'' single-word author names, repeated names).
\end{enumerate}

Pre-screening results are tagged with \texttt{[Pre-screening override]} in the reason string to maintain transparency about which detections come from the pre-screening layer vs.\ the external tool.

\section{Baseline implementation details}
\label{app:baselines}

All baselines are implemented as Python wrappers conforming to the \textsc{Hallmark} baseline interface.
Each wrapper:
(1)~converts \textsc{Hallmark} \texttt{BenchmarkEntry} objects to the tool's expected input format,
(2)~invokes the tool,
(3)~maps the tool's output to a \textsc{Hallmark} \texttt{Prediction} with label, confidence, and reason.

\paragraph{Timeout handling.}
Each baseline is subject to a per-entry timeout (default: 60 seconds).
Entries that timeout are assigned a default prediction of \texttt{VALID} with confidence 0.5, following the conservative assumption that unverifiable entries should not be flagged.

\paragraph{Rate limiting.}
HaRC and verify-citations are subject to Semantic Scholar and Google Scholar rate limits.
For reproducibility, we provide pre-computed reference results in \texttt{data/v1.0/baseline\_results/}, generated by running the tools locally without rate-limit constraints.
CI validates these reference results by checksum rather than re-running the tools.

\section{Temporal analysis}
\label{app:temporal}

We assign entries to three temporal segments: pre-2023 (papers published before January 2023), 2023--2024, and 2025+.
For bibtex-updater, detection rates are consistent across segments ($\pm$2\%), indicating robust performance across publication periods.
DOI-only shows a slight improvement on newer entries, likely because recent papers more consistently include DOIs.
Tools relying on static databases may degrade on newer entries as coverage lags, while live-API tools maintain consistent performance.

\section{Infrastructure documentation}
\label{app:infrastructure}

\textsc{Hallmark} is distributed as a pip-installable Python package with the following components:

\begin{itemize}
    \item \textbf{CLI}: \texttt{hallmark evaluate}, \texttt{hallmark stats}, \texttt{hallmark leaderboard}, \texttt{hallmark contribute}
    \item \textbf{Python API}: \texttt{hallmark.dataset.loader.load\_split()}, \texttt{hallmark.evaluation.metrics.evaluate()}, \texttt{hallmark.evaluation.ranking.rank\_tools()}
    \item \textbf{Baseline registry}: \texttt{hallmark.baselines.registry.\{list\_baselines, check\_available, run\_baseline\}}
    \item \textbf{CI workflows}: \texttt{tests.yml} (test suite across Python 3.10--3.13), \texttt{baselines.yml} (weekly baseline evaluation)
\end{itemize}

\paragraph{Installation.}
\begin{verbatim}
pip install hallmark                    # Core
pip install hallmark[baselines]         # With baseline dependencies
pip install hallmark[ranking]           # With Plackett-Luce support
pip install hallmark[all]               # Everything
\end{verbatim}

\section{Datasheet for \textsc{Hallmark}}
\label{app:datasheet}

Following \citet{gebru2021datasheets}, we provide a datasheet for the \textsc{Hallmark} dataset.

\paragraph{Motivation.}
\textsc{Hallmark} was created to provide a standardized benchmark for evaluating citation hallucination detection tools, motivated by the NeurIPS 2025 incident and subsequent audits.

\paragraph{Composition.}
The dataset contains 2,207 BibTeX entries: 918 valid entries scraped from DBLP and 1,289 hallucinated entries generated through perturbation, LLM generation, adversarial crafting, and real-world collection across 11 main hallucination types plus 3 stress-test types.
Each entry includes 6 binary sub-test labels.

\paragraph{Collection process.}
Valid entries were scraped from the DBLP API and verified against CrossRef and Semantic Scholar.
Hallucinated entries were generated using the methods described in \cref{sec:dataset} and \cref{app:dataset}.

\paragraph{Preprocessing.}
BibTeX records were normalized to a consistent field ordering.
Unicode characters were preserved.
Entries were split into dev/test/hidden sets with stratified sampling across hallucination types and tiers.

\paragraph{Uses.}
\textsc{Hallmark} is intended for evaluating and comparing citation verification tools.
It should not be used to train hallucination generators or to generate convincing fake citations.

\paragraph{Distribution.}
The dataset is distributed under the MIT license via GitHub and PyPI.
The hidden test set is not publicly distributed.

\paragraph{Maintenance.}
The benchmark is maintained by the authors and accepts community contributions through a structured submission process (pull requests to the GitHub repository, validated by automated checks for BibTeX well-formedness, sub-test consistency, and duplicate detection).

\section{Co-designed tool evaluation}
\label{app:codesign}

\texttt{bibtex-updater}~\citep{bibtexupdater} is a multi-database cross-referencing tool developed by the authors of this benchmark.
We report its results separately to maintain transparency about the co-design relationship: the benchmark taxonomy and sub-test structure were informed by the tool's detection capabilities, creating a potential circularity that could inflate its apparent performance.

\begin{table}[h]
\caption{Results for \texttt{bibtex-updater} (co-designed by the authors). Reported separately from independent tools in \cref{tab:results} due to co-design bias concerns.}
\label{tab:codesign}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Tool} & \textbf{DR $\uparrow$} & \textbf{FPR $\downarrow$} & \textbf{F1 $\uparrow$} & \textbf{TW-F1 $\uparrow$} & \textbf{ECE $\downarrow$} & \textbf{Cov.} \\
\midrule
bibtex-updater & 0.954 & 0.024 & 0.962 & 0.969 & 0.452 & 1.00 \\
Ensemble (doi+btx)$^\dagger$ & 0.208 & 0.007 & 0.342 & 0.296 & 0.289 & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

\noindent $^\dagger$The ensemble uses confidence-weighted voting: an entry is flagged as hallucinated only when the weighted vote exceeds a threshold.
Because DOI-only assigns moderate confidence (0.5--0.7) while bibtex-updater assigns high confidence to both hallucinated and valid predictions, the ensemble's threshold filters out many DOI-only detections, producing lower DR than either tool alone.
This conservative behavior reduces false positives (FPR = 0.007) at the cost of recall.

\noindent While \texttt{bibtex-updater} achieves a detection rate of 0.954 (95\% CI: [0.94, 0.97]) and F1 of 0.962, readers should interpret these results with caution:
(1)~the tool's multi-database verification strategy was developed concurrently with the benchmark design;
(2)~the sub-test structure (DOI resolution, title existence, author matching, venue verification) mirrors the tool's internal verification pipeline;
(3)~the pre-screening layer was designed to complement the tool's known blind spots.

Despite these concerns, \texttt{bibtex-updater} serves a useful diagnostic role: its high detection rate provides an upper-bound estimate for what multi-database cross-referencing can achieve, while its poor calibration (ECE = 0.452) reveals that even thorough verification tools produce unreliable confidence scores.
We encourage independent tool developers to submit evaluations via \textsc{Hallmark}'s contribution framework to establish unbiased baselines.

\section{Additional figures}
\label{app:figures}

\begin{figure}[h]
\centering
\includegraphics[width=0.7\linewidth]{figures/overall_comparison.pdf}
\caption{Overall comparison of baseline performance across primary metrics.}
\label{fig:overall}
\end{figure}

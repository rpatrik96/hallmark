\section{Introduction}
\label{sec:introduction}

Citation hallucination---the generation of plausible but fabricated bibliographic references by language models---has emerged as a concrete threat to scientific publishing.
In late 2025, the NeurIPS program committee identified 53 accepted papers containing fabricated citations that had passed peer review~\citep{neurips2025incident}.
Independent audits confirmed the scale of the problem: \citet{ghostcite2026} analyzed 2.2 million citations across 56,000 papers and flagged 604 papers with likely hallucinated references; \citet{hallucitation2026} found approximately 300 affected papers in ACL, NAACL, and EMNLP proceedings alone; and \citet{mysterious2026} documented a sharp rise from zero detected cases in 2021 to consistent problems across HPC venues by 2025.
These findings reveal that citation hallucination is not anecdotal but systematic, and that neither authors nor reviewers reliably catch fabricated references.

Several citation verification tools have been developed in response, ranging from DOI resolution checkers to multi-database cross-referencing systems.
However, \emph{no standardized benchmark exists to compare these tools}: each is evaluated on ad-hoc datasets under different conditions, making it impossible to assess relative strengths, failure modes, or coverage gaps.

\paragraph{Scope.}
\textsc{Hallmark} focuses on citation \emph{metadata} verification---detecting fabricated, inconsistent, or nonexistent bibliographic references.
We do not address claim-level hallucination, where a real paper is cited but its claims are misrepresented.
Claim verification requires full-text analysis and semantic understanding of paper content, representing an important but orthogonal challenge for future work.

We address this gap with four contributions:
\begin{enumerate}
    \item \textbf{A hallucination taxonomy} of 14 types organized into three difficulty tiers (Easy, Medium, Hard), capturing the full spectrum from obviously fake DOIs to subtly plausible fabrications (\cref{sec:taxonomy}).
    \item \textbf{A benchmark dataset} of 2,184 BibTeX entries with six binary sub-tests per entry, enabling multi-criteria evaluation inspired by HumanEval~\citep{humaneval} (\cref{sec:dataset}).
    \item \textbf{An evaluation protocol} with tier-weighted F1 (rewarding detection of harder hallucinations), expected calibration error (measuring confidence accuracy), and Plackett-Luce ranking~\citep{luce1959,plackett1975} for comparing tools with incomplete coverage (\cref{sec:evaluation}).
    \item \textbf{Open infrastructure}: a pip-installable Python package with a baseline registry, CI-integrated evaluation pipeline, and a community contribution system inspired by ONEBench~\citep{onebench2024} (\cref{sec:experiments}).
\end{enumerate}

\begin{table}[t]
\caption{Comparison of \textsc{Hallmark} with related citation analysis efforts. Prior work focuses on auditing published papers; \textsc{Hallmark} is the first \emph{evaluation benchmark} for citation verification tools.}
\label{tab:comparison}
\centering
\small
\begin{tabular}{lccccc}
\toprule
& \textbf{Entries} & \textbf{Types} & \textbf{Sub-tests} & \textbf{Tool eval.} & \textbf{Open} \\
\midrule
\multicolumn{6}{l}{\emph{Citation audits}} \\
GhostCite~\citep{ghostcite2026} & 56K papers & --- & \ding{55} & \ding{55} & \ding{55} \\
HalluCitation~\citep{hallucitation2026} & 300 papers & 3 & \ding{55} & \ding{55} & \ding{51} \\
Mysterious Citations~\citep{mysterious2026} & HPC venues & --- & \ding{55} & \ding{55} & \ding{55} \\
\midrule
\multicolumn{6}{l}{\emph{Citation verification benchmarks}} \\
\textsc{Hallmark} (ours) & 2,184 & 14 & 6 & \ding{51} & \ding{51} \\
\bottomrule
\end{tabular}
\end{table}

We evaluate five tools on \textsc{Hallmark}, including bibtex-updater, HaRC, verify-citations, a DOI-only baseline, and a conservative ensemble.
The evaluation reveals that no single tool covers all 14 hallucination types, calibration varies widely (ECE from 0.11 to 0.45), and all tools share blind spots on venue-level hallucinations requiring metadata cross-referencing.
These results demonstrate the value of a structured benchmark for identifying actionable improvement targets.

\section{Introduction}
\label{sec:introduction}

Citation hallucination---the generation of plausible but fabricated bibliographic references by language models---has emerged as a concrete threat to scientific publishing.
In late 2025, the NeurIPS program committee identified 53 accepted papers containing fabricated citations that had passed peer review~\citep{neurips2025incident}.
Independent audits confirmed the scale of the problem: \citet{ghostcite2026} analyzed 2.2 million citations across 56,000 papers and flagged 604 papers with likely hallucinated references; \citet{hallucitation2026} found approximately 300 affected papers in ACL, NAACL, and EMNLP proceedings alone; and \citet{mysterious2026} documented a sharp rise from zero detected cases in 2021 to consistent problems across HPC venues by 2025.
These findings reveal that citation hallucination is not anecdotal but systematic, and that neither authors nor reviewers reliably catch fabricated references.

Several citation verification tools have been developed in response, ranging from DOI resolution checkers to multi-database cross-referencing systems.
However, two gaps remain.
First, \emph{no standardized benchmark exists to compare these tools}: each is evaluated on ad-hoc datasets under different conditions, making it impossible to assess relative strengths, failure modes, or coverage gaps.
Second, \emph{no existing tool is designed for venue-scale deployment}: current tools are built for individual researchers, not for program committees processing hundreds of submissions under time constraints.

We address both gaps with four core benchmark contributions, plus a reference implementation to validate the benchmark's utility:
\begin{enumerate}
    \item \textbf{A hallucination taxonomy} of 13 types organized into three difficulty tiers (Easy, Medium, Hard), capturing the full spectrum from obviously fake DOIs to subtly plausible fabrications (\cref{sec:taxonomy}).
    \item \textbf{A benchmark dataset} of 1,700 BibTeX entries (800 hallucinated, 900 valid; 1,500 public split: 840 dev + 660 test) with six binary sub-tests per entry, enabling multi-criteria evaluation inspired by HumanEval~\citep{humaneval} (\cref{sec:dataset}).
    \item \textbf{An evaluation protocol} with tier-weighted F1 (giving extra credit for detecting harder hallucinations), expected calibration error or ECE (measuring whether confidence scores track actual accuracy), and Plackett-Luce ranking~\citep{luce1959,plackett1975} (a probabilistic model handling tools evaluated on different entry subsets) for comparing tools with incomplete coverage (\cref{sec:evaluation}).
    \item \textbf{Open infrastructure}: a pip-installable Python package with a baseline registry, CI-integrated evaluation pipeline, and a community contribution system inspired by ONEBench~\citep{onebench2024} (\cref{sec:experiments}).
\end{enumerate}

To validate the benchmark's utility, we also contribute \texttt{bibtex-updater}: a reference implementation of a citation verification tool designed for venue-scale deployment---integration into CI/CD workflows, pre-commit hooks, and batch submission processing---rather than benchmark optimization (\cref{sec:tool}).
While bibtex-updater was developed by the same authors as the benchmark, it serves to demonstrate that \textsc{Hallmark} supports practical tool evaluation and reveals actionable gaps.
We acknowledge this potential source of benchmark design bias and mitigate it by: (1) evaluating multiple independent baselines alongside bibtex-updater, (2) designing the taxonomy from publicly documented incidents rather than bibtex-updater's failure modes, (3) open-sourcing all data and code to enable community scrutiny, and (4) reporting bibtex-updater performance with and without the benchmark's pre-screening layer to isolate the tool's contribution from shared infrastructure.

\begin{table}[t]
\caption{Comparison of \textsc{Hallmark} with related citation analysis efforts. \textsc{Hallmark} is the first \emph{benchmark} for citation verification tool evaluation, whereas prior work focuses on auditing or detection.}
\label{tab:comparison}
\centering
\small
\begin{tabular}{lccccc}
\toprule
& \textbf{Entries} & \textbf{Types} & \textbf{Sub-tests} & \textbf{Tool eval.} & \textbf{Open} \\
\midrule
GhostCite~\citep{ghostcite2026} & 56K papers & --- & \ding{55} & \ding{55} & \ding{55} \\
HalluCitation~\citep{hallucitation2026} & 300 papers & 3 & \ding{55} & \ding{55} & \ding{51} \\
Mysterious Citations~\citep{mysterious2026} & HPC venues & --- & \ding{55} & \ding{55} & \ding{55} \\
\midrule
\textsc{Hallmark} (ours) & 1,700 & 13 & 6 & \ding{51} & \ding{51} \\
\bottomrule
\end{tabular}
\end{table}

Our evaluation shows that \texttt{bibtex-updater} achieves 0.95 detection rate (95\% CI: [0.94, 0.97]) and 0.96 F1 overall, with well-calibrated confidence scores among all tools.
Its performance drops on medium-tier types requiring venue-level metadata verification---a gap shared by all evaluated tools.
No single tool covers all 13 hallucination types, and calibration varies widely (ECE from 0.11 to 0.45).
These results demonstrate the value of a structured benchmark for identifying actionable improvement targets, and the feasibility of venue-scale automated verification with current tools.

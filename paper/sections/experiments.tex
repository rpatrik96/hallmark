\section{Experiments}
\label{sec:experiments}

\subsection{Evaluated tools}
\label{sec:baselines}

We evaluate three independent citation verification tools on the \texttt{dev\_public} split.
We additionally report results for \texttt{bibtex-updater}, a tool developed by the authors, in \cref{app:codesign} to maintain transparency about the co-design relationship.

\paragraph{DOI-only.}
A minimal baseline that checks whether each entry's DOI field resolves via the CrossRef API.
Entries with a non-resolving DOI are flagged as hallucinated; entries without a DOI field are conservatively assumed valid.
This baseline tests the lower bound of what simple metadata checks can achieve.

\paragraph{HaRC.}
The Hallucinated Reference Checker~\citep{harc2024} queries Semantic Scholar, DBLP, Google Scholar, and Open Library.
It uses a multi-stage pipeline: DOI lookup, title search, author verification, and venue cross-check.
Due to Semantic Scholar API rate limiting, HaRC completed evaluation on only 20 of 903 dev entries (2.2\%) within our timeout budget.

\paragraph{verify-citations.}
A pip-installable tool~\citep{verifycitations2025} that queries arXiv, ACL Anthology, Semantic Scholar, DBLP, Google Scholar, and DuckDuckGo.
Like HaRC, it was rate-limited and completed 71 of 903 entries (7.9\%).

\subsection{Main results}
\label{sec:main_results}

\cref{tab:results} presents the main results on the \texttt{dev\_public} split.

\begin{table}[t]
\caption{Results on \texttt{dev\_public} using independent tools only. Best values in \textbf{bold}. $^*$Partial evaluation due to API rate limiting. Coverage = fraction of entries processed. Results for bibtex-updater (co-designed by the authors) are reported separately in \cref{app:codesign}.}
\label{tab:results}
\centering
\small
\begin{tabular}{lcccccc}
\toprule
\textbf{Tool} & \textbf{DR $\uparrow$} & \textbf{FPR $\downarrow$} & \textbf{F1 $\uparrow$} & \textbf{TW-F1 $\uparrow$} & \textbf{ECE $\downarrow$} & \textbf{Cov.} \\
\midrule
DOI-only            & \textbf{0.223} [0.19, 0.25] & 0.178          & \textbf{0.312} [0.27, 0.35] & \textbf{0.294}          & \textbf{0.111} & 1.00 \\
HaRC$^*$            & 0.155          & \textbf{0.000} & 0.268          & 0.188          & 0.361          & 0.02 \\
verify-citations$^*$& 0.042          & 0.024          & 0.071          & 0.062          & 0.317          & 0.08 \\
\bottomrule
\end{tabular}
\end{table}

The results reveal three key insights about the current citation verification landscape.
First, \emph{DOI resolution alone is insufficient}: the DOI-only baseline catches only 22.3\% of hallucinations because most types in \textsc{Hallmark} use valid DOIs---only \texttt{fabricated\_doi} entries are caught by DOI resolution alone.
Second, \emph{API rate limiting severely constrains multi-source tools}: HaRC and verify-citations completed only 2.2\% and 7.9\% of entries respectively, making them impractical for venue-scale deployment without caching infrastructure.
Third, \emph{no independent tool achieves satisfactory performance}: the best independent tool (DOI-only) detects fewer than one in four hallucinations, exposing a critical gap in the current tooling ecosystem.
Since Detection Rate and FPR are prevalence-independent, we emphasize these as primary metrics.\footnote{F1 is prevalence-sensitive: a benchmark with 50\% hallucination prevalence inflates F1 relative to real-world deployment where hallucination rates may be much lower. F1 values should only be compared within the same split, never across splits or with external results.}

\paragraph{Incomplete-coverage tools.}
HaRC and verify-citations completed only 2.2\% and 7.9\% of entries respectively due to API rate limiting.
Their metrics reflect performance on the completed subset only and are not directly comparable to full-coverage tools.
The framework supports LLM-based baselines (e.g., GPT-4o, Claude), which would provide independent high-coverage comparisons and likely represent the most practical deployment path for venues.
We did not evaluate them in this work due to API cost constraints but provide a reference implementation; we encourage the community to submit such evaluations.
Plackett-Luce ranking (\cref{app:ranking}) provides a principled framework for comparing tools with heterogeneous coverage but requires substantially higher pairwise overlap than achieved here.

We compute 95\% confidence intervals using stratified bootstrap (10{,}000 resamples, stratified by hallucination type).
With $\geq$29 instances per type, per-type metrics have narrower confidence intervals than the original 10-instance design; we provide exact counts and power analysis in \cref{app:statistics}.

\subsection{Per-tier analysis}
\label{sec:per_tier}

\cref{fig:tier} shows detection rates broken down by difficulty tier.
DOI-only detection is concentrated in Tier~1, as expected, with near-zero performance on Tiers~2 and~3.
Neither HaRC nor verify-citations achieve meaningful coverage at any tier due to rate limiting.
The tier structure exposes a clear gap: no independent tool addresses Tier~2 and~3 hallucinations, which require cross-referencing metadata fields and semantic reasoning---capabilities absent from current publicly available tools.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/tier_detection_rates.pdf}
\caption{Detection rate by difficulty tier for independent tools. DOI-only detection is concentrated in Tier~1; no independent tool addresses Tier~2 or~3.}
\label{fig:tier}
\end{figure}

\subsection{Per-type analysis}
\label{sec:per_type}

\cref{fig:heatmap} shows the per-type detection heatmap across all evaluated tools.
With $\geq$29 instances per type, per-type breakdowns provide statistically meaningful comparisons.
DOI-only detects \texttt{fabricated\_doi} reliably but misses all types that use valid DOIs.
No independent tool addresses the 7 types in Tiers~2--3 that require cross-referencing or semantic reasoning.
This gap motivates the development of more sophisticated verification tools---and LLM-based approaches are a natural candidate, which we encourage the community to evaluate using \textsc{Hallmark}.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/per_type_heatmap.pdf}
\caption{Per-type detection rate across independent tools. Each cell shows the detection rate for a specific hallucination type. DOI-only addresses only \texttt{fabricated\_doi}; no independent tool covers Tier~2--3 types.}
\label{fig:heatmap}
\end{figure}

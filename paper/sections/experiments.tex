\section{Experiments}
\label{sec:experiments}

\subsection{Evaluated tools}
\label{sec:baselines}

We evaluate three independent external citation verification tools (HaRC, verify-citations, GPT-5.1) plus DOI-only, an internal lower-bound baseline, on the \texttt{dev\_public} split.
We additionally report results for \texttt{bibtex-updater}, a tool developed by the authors, in \cref{app:codesign} to maintain transparency about the co-design relationship.

\paragraph{DOI-only.}
A minimal baseline that checks whether each entry's DOI field resolves via the CrossRef API.
Entries with a non-resolving DOI are flagged as hallucinated; entries without a DOI field are conservatively assumed valid.
Results in \cref{tab:results} include the shared pre-screening layer (\cref{sec:prescreening}); without pre-screening, overall detection rate drops by ${\sim}5$pp (from 25.6\% to 20.3\%), with the largest effect on Tier~1 (${\sim}18$pp).
This baseline tests the lower bound of what simple metadata checks can achieve.

\paragraph{HaRC.}
The Hallucinated Reference Checker~\citep{harc2024} queries Semantic Scholar, DBLP, Google Scholar, and Open Library.
It uses a multi-stage pipeline: DOI lookup, title search, author verification, and venue cross-check.
Due to Semantic Scholar API rate limiting, HaRC completed evaluation on only 20 of 1,119 dev entries (1.8\%) within our timeout budget.

\paragraph{verify-citations.}
A pip-installable tool~\citep{verifycitations2025} that queries arXiv, ACL Anthology, Semantic Scholar, DBLP, Google Scholar, and DuckDuckGo.
Like HaRC, it was rate-limited and completed 71 of 1,119 entries (6.3\%).

\subsection{Main results}
\label{sec:main_results}

\cref{tab:results} presents the main results on the \texttt{dev\_public} split.

\begin{table}[t]
\caption{Results on \texttt{dev\_public} using independent tools only. Best values in \textbf{bold}. $^*$Partial evaluation due to API rate limiting. Coverage = fraction of entries processed. Results for bibtex-updater (co-designed by the authors) are reported separately in \cref{app:codesign}.}
\label{tab:results}
\centering
\small
\begin{tabular}{lccccccc}
\toprule
\textbf{Tool} & \textbf{DR $\uparrow$} & \textbf{FPR $\downarrow$} & \textbf{F1 $\uparrow$} & \textbf{MCC $\uparrow$} & \textbf{TW-F1 $\uparrow$} & \textbf{ECE $\downarrow$} & \textbf{Cov.} \\
\midrule
DOI-only            & 0.256          & 0.195          & 0.361          & 0.093          & 0.314          & 0.143 & 1.00 \\
HaRC$^*$            & 0.155          & \textbf{0.000} & 0.268          & 0.234          & 0.188          & 0.361          & 0.02 \\
verify-citations$^*$& 0.042          & 0.024          & 0.071          & 0.032          & 0.062          & 0.317          & 0.06 \\
\midrule
GPT-5.1 (zero-shot) & \textbf{0.797} & 0.171          & \textbf{0.822} & \textbf{0.636} & \textbf{0.846} & \textbf{0.107} & 1.00 \\
\bottomrule
\end{tabular}
\end{table}

The results reveal three key insights about the current citation verification landscape.
First, \emph{API-based tools are insufficient}: the best API-based tool (DOI-only) catches only 25.6\% of hallucinations because most types in \textsc{Hallmark} use valid DOIs, and multi-source tools (HaRC, verify-citations) completed only 1.8\% and 6.3\% of entries respectively due to rate limiting.
Second, \emph{LLMs dramatically outperform API-based tools}: GPT-5.1 achieves an 80\% detection rate---a $3{\times}$ improvement over DOI-only---demonstrating that language models can identify the very hallucinations they produce.
Third, \emph{a capability gap remains}: even GPT-5.1 misses 20\% of hallucinations, with systematic weaknesses on subtle types like \texttt{author\_mismatch} (63\%) and \texttt{near\_miss\_title} (63\%), providing concrete improvement targets.
Since Detection Rate and FPR are prevalence-independent, we emphasize these as primary metrics.\footnote{F1 is prevalence-sensitive: a benchmark with 50\% hallucination prevalence inflates F1 relative to real-world deployment where hallucination rates may be much lower. F1 values should only be compared within the same split, never across splits or with external results.}

\paragraph{GPT-5.1 (zero-shot).}
We evaluate GPT-5.1~\citep{openai2025gpt5} as a zero-shot citation verifier: each BibTeX entry is presented with a prompt asking the model to classify it as \texttt{VALID} or \texttt{HALLUCINATED} with a confidence score.
Unlike API-dependent tools, the LLM baseline achieves full coverage (100\% of entries) and the lowest ECE (0.107), indicating well-calibrated confidence estimates.

\paragraph{Incomplete-coverage tools.}
HaRC and verify-citations completed only 1.8\% and 6.3\% of entries respectively due to API rate limiting.
Their metrics reflect performance on the completed subset only and are not directly comparable to full-coverage tools.
Plackett-Luce ranking (\cref{app:ranking}) provides a principled framework for comparing tools with heterogeneous coverage but requires substantially higher pairwise overlap than achieved here.

We compute 95\% confidence intervals using stratified bootstrap (10{,}000 resamples, stratified by hallucination type).
With $\geq$30 instances per type, per-type metrics have narrower confidence intervals than the original 10-instance design; we provide exact counts and power analysis in \cref{app:statistics}.

\subsection{Per-tier analysis}
\label{sec:per_tier}

\cref{fig:tier} shows detection rates broken down by difficulty tier.
DOI-only detection is concentrated in Tier~1, as expected, with near-zero performance on Tiers~2 and~3.
GPT-5.1 achieves strong performance across all tiers (Tier~1: 87\%, Tier~2: 76\%, Tier~3: 79\%), with graceful degradation as difficulty increases---consistent with the tier design.
The tier structure exposes the capability gap between API-based and LLM-based approaches: API tools lack the cross-referencing and semantic reasoning needed for Tier~2--3 hallucinations, while LLMs bring parametric knowledge that compensates for missing API coverage.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figures/tier_detection_rates.pdf}
\caption{Detection rate by difficulty tier. DOI-only detection is concentrated in Tier~1; GPT-5.1 achieves strong performance across all tiers with graceful degradation.}
\label{fig:tier}
\end{figure}

\subsection{Per-type analysis}
\label{sec:per_type}

\cref{fig:heatmap} shows the per-type detection heatmap across all evaluated tools.
With $\geq$30 instances per type, per-type breakdowns provide statistically meaningful comparisons.
DOI-only detects \texttt{fabricated\_doi} reliably but misses all types that use valid DOIs.
GPT-5.1 achieves ${\geq}67\%$ detection on 9 of 11 main types, with near-perfect detection of \texttt{nonexistent\_venue} (97\%) and \texttt{placeholder\_authors} (94\%).
Its weakest types---\texttt{author\_mismatch} (63\%) and \texttt{near\_miss\_title} (63\%)---involve subtle metadata perturbations that require exact bibliographic knowledge, providing concrete targets for future work.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figures/per_type_heatmap.pdf}
\caption{Per-type detection rate across evaluated tools. Each cell shows the detection rate for a specific hallucination type. GPT-5.1 covers most types; \texttt{author\_mismatch} and \texttt{near\_miss\_title} remain challenging.}
\label{fig:heatmap}
\end{figure}

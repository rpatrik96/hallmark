\section{Limitations}
\label{sec:limitations}

At 2,229 entries (1,311 hallucinated, with at least 30 instances per type per public split), the dataset provides improved statistical power per type but remains small relative to the full diversity of possible citation hallucinations.
With 30 instances per type per split, per-type metrics have narrower confidence intervals than the original 10-instance design (e.g., a type-level detection rate of 90\% has a 95\% Clopper-Pearson interval of [0.74, 0.98] rather than [0.55, 1.00]); we report per-type breakdowns with increased statistical confidence.
The hallucination prevalence differs across splits (54.1\% in dev, 64.8\% in test) due to scaling hallucinated entries independently per split and the addition of LLM-generated/real-world entries; prevalence-sensitive metrics should be interpreted with this caveat.
The benchmark covers only English-language BibTeX references.
The majority of hallucinated entries are synthetically generated via perturbation, though the benchmark now includes 222 LLM-generated entries (produced by prompting GPT-5.1 to generate plausible citations and verifying against CrossRef/DBLP with title similarity $\geq 85$ and author Jaccard $\geq 0.5$) and 114 real-world entries harvested from documented incidents (97 from GPTZero's NeurIPS 2025 analysis, plus entries from GhostCite and HalluCitation).
These additions improve ecological validity; the LLM-generated entries concentrate on \texttt{plausible\_fabrication} (40\%) and \texttt{fabricated\_doi} (33\%), reflecting that LLM-produced citations predominantly cannot be verified in any database.
The real-world sample remains small.
Future versions should expand real-world coverage as more documented incidents become available.
Baseline performance depends on bibliographic API availability and coverage; results may shift as APIs evolve.
We did not evaluate commercial plagiarism detection tools (Turnitin, iThenticate), which focus on textual similarity rather than citation metadata verification.
Valid entries are drawn from 2018--2025 ML venues and may not generalize to other fields or time periods.
The community contribution system is designed to address these coverage limitations over time.
All synthetic entries are generated with a single fixed seed; we did not conduct seed-sensitivity analysis, though the deterministic generation ensures exact reproducibility.

\paragraph{DOI-presence distributional artifact.}
After DBLP-based DOI enrichment, approximately 36\% of valid entries lack DOI fields (down from 62\%), reducing the distributional gap with \texttt{fabricated\_doi} entries (which contain DOIs by definition).
While DOI presence still carries some signal for this hallucination type, the enrichment substantially mitigates this artifact.
Per-type metrics (\cref{sec:per_type}) enable practitioners to assess tool performance separately on types with and without this residual artifact.

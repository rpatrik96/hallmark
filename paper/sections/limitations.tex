\section{Limitations}
\label{sec:limitations}

At 2,560 entries (1,580 hallucinated, with at least 30 instances per type per public split), the dataset provides statistical power per type but remains small relative to the full diversity of possible citation hallucinations.
With $\geq$30 instances per type per split, per-type 95\% Clopper-Pearson confidence intervals have width $\leq 0.24$ at detection rates near 0 or 1 (e.g., DR $= 0.90$ with $n=30$ yields CI $[0.74, 0.98]$); at DR $= 0.50$ the width increases to $\approx 0.37$.
A two-proportion $z$-test with $n=30$ per type achieves 80\% power to detect a detection rate difference of $\geq 0.20$ between tools ($\alpha = 0.05$, two-sided).
Hallucination prevalence differs across main splits (54.5\% in dev, 65.0\% in test, 56.0\% in hidden) due to the constraint that each of 14 types requires $\geq 30$ instances per main split.
We emphasize prevalence-independent metrics (Detection Rate, FPR) as primary; prevalence-sensitive metrics (F1) should be compared within splits only.
The benchmark covers only English-language BibTeX references.
The majority of hallucinated entries are synthetically generated via perturbation, though the benchmark includes 284 LLM-generated entries (produced by prompting GPT-5.1 to generate plausible citations and verifying against CrossRef with automated type classification) and 97 curated entries from GPTZero's NeurIPS 2025 analysis plus 13 real-world entries harvested from documented incidents (GhostCite, HalluCitation).
The real-world sample remains small and type-skewed (55\% \texttt{plausible\_fabrication}).
Future versions should expand real-world coverage as more documented incidents become available.
Baseline performance depends on bibliographic API availability and coverage; results may shift as APIs evolve.
We did not evaluate commercial plagiarism detection tools (Turnitin, iThenticate), which focus on textual similarity rather than citation metadata verification.
Valid entries are drawn from 2021--2023 ML venues and may not generalize to other fields or time periods.
The community contribution system is designed to address these coverage limitations over time.
All synthetic entries are generated with a fixed seed for reproducibility.
Seed-sensitivity analysis across five seeds shows 95--100\% entry variation per generator, confirming that the pools are large enough to avoid deterministic repetition.

\paragraph{LLM knowledge cutoff.}
LLM-based verifiers carry an implicit temporal bias: their parametric knowledge has a training data cutoff, beyond which genuine papers become unrecognizable.
Our temporal probe (\cref{sec:temporal_robustness}) shows GPT-5.1's false positive rate rises from 17\% to 63\% on 2024--2026 papers, meaning it flags most recent valid entries as hallucinated.
Performance metrics therefore depend on the temporal overlap between an LLM's training data and the test entries.
As models are updated, their effective cutoff shifts, making temporal robustness a moving target that benchmarks must track continuously.

\paragraph{Benchmark-tool co-design.}
As the authors of both the benchmark and one evaluated tool (bibtex-updater), we acknowledge potential co-design bias.
We mitigate this by reporting bibtex-updater results separately in \cref{app:codesign}, evaluating all tools with identical pre-screening infrastructure, releasing the full evaluation protocol for independent replication, and actively soliciting independent tool submissions via the benchmark's public leaderboard.

\paragraph{Domain transfer.}
\textsc{Hallmark} covers English-language ML references from five top venues (NeurIPS, ICML, ICLR, AAAI, CVPR).
Practitioners in other fields can use the evaluation framework with domain-specific valid entries; the hallucination taxonomy and sub-test definitions are domain-agnostic.
We provide reweighting utilities to adjust hallucination type prevalence to observed real-world distributions.

\paragraph{DOI-presence distributional artifact.}
Approximately 38\% of valid entries in the dev split lack DOI fields.
While DOI presence carries some signal for the \texttt{fabricated\_doi} hallucination type (which contains DOIs by definition), per-type metrics (\cref{sec:per_type}) enable practitioners to assess tool performance separately on types with and without this residual artifact.

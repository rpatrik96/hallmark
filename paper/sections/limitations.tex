\section{Limitations}
\label{sec:limitations}

At 2,207 entries (1,289 hallucinated, with at least 29 instances per type per public split), the dataset provides statistical power per type but remains small relative to the full diversity of possible citation hallucinations.
With $\geq$29 instances per type per split, per-type 95\% Clopper-Pearson confidence intervals have width $\leq 0.25$ at detection rates near 0 or 1 (e.g., DR $= 0.90$ with $n=29$ yields CI $[0.73, 0.98]$); at DR $= 0.50$ the width increases to $\approx 0.38$.
A two-proportion $z$-test with $n=29$ per type achieves 80\% power to detect a detection rate difference of $\geq 0.20$ between tools ($\alpha = 0.05$, two-sided).
Hallucination prevalence differs across main splits (50.2\% in dev, 57.3\% in test, 57.8\% in hidden) due to the constraint that each of 11 main types requires $\geq 29$ instances per main split.
We emphasize prevalence-independent metrics (Detection Rate, FPR) as primary; prevalence-sensitive metrics (F1) should be compared within splits only.
The benchmark covers only English-language BibTeX references.
The majority of hallucinated entries are synthetically generated via perturbation, though the benchmark includes 162 LLM-generated entries (produced by prompting GPT-5.1 to generate plausible citations and verifying against CrossRef/DBLP with title similarity $\geq 85\%$ and author Jaccard $\geq 0.5$) and 72 real-world entries harvested from documented incidents (primarily from GPTZero's NeurIPS 2025 analysis, GhostCite, and HalluCitation).
The real-world sample remains small and type-skewed (55\% \texttt{plausible\_fabrication}).
Future versions should expand real-world coverage as more documented incidents become available.
Baseline performance depends on bibliographic API availability and coverage; results may shift as APIs evolve.
We did not evaluate commercial plagiarism detection tools (Turnitin, iThenticate), which focus on textual similarity rather than citation metadata verification.
Valid entries are drawn from 2021--2023 ML venues and may not generalize to other fields or time periods.
The community contribution system is designed to address these coverage limitations over time.
All synthetic entries are generated with a fixed seed for reproducibility.
Seed-sensitivity analysis across five seeds shows 95--100\% entry variation per generator, confirming that the pools are large enough to avoid deterministic repetition.

\paragraph{Benchmark-tool co-design.}
As the authors of both the benchmark and one evaluated tool (bibtex-updater), we acknowledge potential co-design bias.
We mitigate this by reporting bibtex-updater results separately in \cref{app:codesign}, evaluating all tools with identical pre-screening infrastructure, releasing the full evaluation protocol for independent replication, and actively soliciting independent tool submissions via the benchmark's public leaderboard.

\paragraph{Domain transfer.}
\textsc{Hallmark} covers English-language ML references from five top venues (NeurIPS, ICML, ICLR, AAAI, CVPR).
Practitioners in other fields can use the evaluation framework with domain-specific valid entries; the hallucination taxonomy and sub-test definitions are domain-agnostic.
We provide reweighting utilities to adjust hallucination type prevalence to observed real-world distributions.

\paragraph{DOI-presence distributional artifact.}
Approximately 38\% of valid entries in the dev split lack DOI fields.
While DOI presence carries some signal for the \texttt{fabricated\_doi} hallucination type (which contains DOIs by definition), per-type metrics (\cref{sec:per_type}) enable practitioners to assess tool performance separately on types with and without this residual artifact.

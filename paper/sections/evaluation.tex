\section{Evaluation protocol}
\label{sec:evaluation}

\subsection{Metrics}
\label{sec:metrics}

\textsc{Hallmark} reports five primary metrics and several diagnostic metrics.

\paragraph{Primary metrics.}
\textbf{Detection Rate (DR)} is recall on the hallucinated class: the fraction of hallucinated entries correctly flagged.
\textbf{False Positive Rate (FPR)} measures the fraction of valid entries incorrectly flagged as hallucinated---critical for practical deployment where false alarms erode user trust.
\textbf{F1-Hallucination} is the harmonic mean of precision and recall on the hallucinated class.

\textbf{Tier-weighted F1 (TW-F1)} addresses a key limitation of standard F1: it treats all hallucinations equally, though detecting a plausible fabrication is harder and arguably more valuable than catching a fabricated DOI.
TW-F1 weights each hallucinated entry's contribution to precision and recall by its tier: Tier~1 entries contribute weight 1, Tier~2 weight 2, and Tier~3 weight 3.
Formally, for entries $\{e_i\}$ with tier weights $w_i \in \{1,2,3\}$, predictions $\hat{y}_i$, and labels $y_i$:
\begin{align}
\text{TW-Precision} &= \frac{\sum_i w_i \cdot \mathbf{1}[\hat{y}_i = y_i = \text{H}]}{\sum_i w_i \cdot \mathbf{1}[\hat{y}_i = \text{H}]}, &
\text{TW-Recall} &= \frac{\sum_i w_i \cdot \mathbf{1}[\hat{y}_i = y_i = \text{H}]}{\sum_i w_i \cdot \mathbf{1}[y_i = \text{H}]}, \label{eq:twf1}
\end{align}
where H denotes the hallucinated class, and TW-F1 is their harmonic mean.

\textbf{Expected Calibration Error (ECE)}~\citep{naeini2015} measures how well a tool's confidence scores reflect its actual accuracy.
We partition predictions into $B=10$ equal-width confidence bins and compute:
\begin{equation}
\text{ECE} = \sum_{b=1}^{B} \frac{|S_b|}{N} \left| \text{acc}(S_b) - \text{conf}(S_b) \right|, \label{eq:ece}
\end{equation}
where $S_b$ is the set of predictions in bin $b$, $\text{acc}(S_b)$ is the fraction of correct predictions, and $\text{conf}(S_b)$ is the mean confidence.
Well-calibrated tools have ECE $\approx 0$.

\paragraph{Diagnostic metrics.}
\textbf{detect@$k$} is the fraction of hallucinations detected by at least one of $k$ verification strategies, analogous to HumanEval's pass@$k$~\citep{humaneval}.
\textbf{Per-tier and per-type breakdowns} reveal which categories each tool handles well or poorly.
\textbf{Source-stratified metrics} disaggregate performance by which bibliographic APIs a tool queried, revealing API-specific blind spots.
\textbf{Sub-test accuracy} measures per-sub-test correctness, showing whether a tool detects hallucinations for the right reasons.

\subsection{Ranking with incomplete data}
\label{sec:ranking}

Not all tools can process all entries---rate-limited API access, timeouts, and tool-specific constraints mean evaluation matrices are typically sparse.
Following ONEBench~\citep{onebench2024}, we adopt the Plackett-Luce model~\citep{luce1959,plackett1975} for ranking tools from incomplete data.

The model assigns each tool $j$ a strength parameter $\theta_j > 0$.
Given a set of pairwise comparisons derived from the results matrix (tool $j$ beats tool $k$ on entry $i$ if it scores higher), we estimate $\{\theta_j\}$ via the Iterative Luce Spectral Ranking (ILSR) algorithm~\citep{maystre2015}.
The resulting parameters yield a principled ranking even when different tools have been evaluated on different subsets of entries.
We also report a simpler mean-score ranking as a baseline comparison.

\subsection{Baseline integration}
\label{sec:baseline_integration}

\textsc{Hallmark} provides a baseline registry that supports discovery, availability checking, and dispatch for all integrated tools.
New baselines register via a decorator pattern, specifying their dependencies and whether they require API keys.
A CI workflow runs all free baselines weekly on the dev split, ensuring results remain reproducible as APIs evolve.
Rate-limited baselines use pre-computed reference results validated by checksum, enabling CI to verify consistency without re-running expensive API calls.

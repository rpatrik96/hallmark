\section{Analysis}
\label{sec:analysis}

\paragraph{Pre-screening effect.}
\label{sec:prescreening}
\textsc{Hallmark} includes an optional pre-screening layer---DOI format validation, year-range bounds checking, and author-name heuristics---that runs before invoking external tools.
This is benchmark infrastructure available to \emph{all} baselines, not a tool-specific feature.
Pre-screening overlaps with bibtex-updater's built-in checks but provides incremental lift (${\sim}5$pp on Tier~1 types).
We report results both with and without pre-screening to maintain transparency about which detections come from the tool versus benchmark infrastructure.

\paragraph{Calibration.}
\label{sec:calibration}
DOI-only achieves the lowest ECE (0.111), with \texttt{bibtex-updater} at 0.452, meaning confidence calibration varies across tools---a critical property for venue deployment, where reviewers need to trust flagged entries without manual verification.
HaRC (0.361), Ensemble (0.289), and verify-citations (0.317) show intermediate calibration, reporting varying confidence on correct and incorrect predictions.
Poor calibration limits practical utility: a tool reporting 80\% confidence regardless of correctness provides no actionable signal to a program committee.

\paragraph{Cost--accuracy tradeoff.}
\label{sec:cost}
\cref{fig:cost} plots detection rate against cost.
\texttt{bibtex-updater} queries 2.8 APIs per entry at 3.1 entries/second---strong cost efficiency for its 95.4\% detection rate.
At this throughput, a venue receiving 3,000 submissions with 50 references each can verify all citations in approximately 13.5 hours with a single worker, or under 2 hours with 8 concurrent workers.
DOI-only is cheapest (1 API call) but achieves only 22.3\%.
HaRC and verify-citations are bottlenecked by rate limiting, making them impractical for venue-scale use.

\begin{figure}[t]
\centering
\includegraphics[width=0.65\linewidth]{figures/cost_accuracy.pdf}
\caption{Cost--accuracy tradeoff. \texttt{bibtex-updater} achieves the best balance of detection rate and throughput; rate-limited tools are impractical for venue-scale deployment.}
\label{fig:cost}
\end{figure}

\paragraph{Synthetic vs.\ real-world representativeness.}
\label{sec:synth_vs_real}
To validate that perturbation-generated entries behave similarly to real-world hallucinations, we compare feature distributions (author count, title length, year) across the seven hallucination types that have both real-world and synthetic instances.
Kolmogorov-Smirnov tests show that 4 of 7 types are statistically indistinguishable ($p > 0.05$ on all features).
The three divergent types (\texttt{near\_miss\_title}, \texttt{plausible\_fabrication}, \texttt{chimeric\_title}) differ primarily in author count---real-world entries average 1.5--2.0 authors versus 3.3--4.7 for synthetic entries, reflecting that real LLM hallucinations tend to fabricate shorter author lists.
Title length and year distributions match across all types.
These results support using perturbation entries as controlled diagnostic tests while motivating continued expansion of the real-world sample.

\paragraph{Failure modes and improvement targets.}
\label{sec:failure}
\texttt{bibtex-updater} shows reduced performance on \texttt{preprint\_as\_published} and \texttt{wrong\_venue}---both venue-level hallucinations.
Current APIs do not reliably distinguish ``published at venue~X'' from ``available on arXiv,'' nor expose structured venue-to-paper mappings.
These gaps are not unique to our tool---no evaluated baseline detects these types reliably---but they represent concrete improvement targets.
\textsc{Hallmark}'s per-type analysis makes these gaps visible and measurable, guiding both tool development and advocacy for richer bibliographic APIs.

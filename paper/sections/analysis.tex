\section{Analysis}
\label{sec:analysis}

\paragraph{Pre-screening effect.}
\label{sec:prescreening}
\textsc{Hallmark} includes an optional pre-screening layer---DOI format validation, year-range bounds checking, and author-name heuristics---that runs before invoking external tools.
This is benchmark infrastructure available to \emph{all} baselines, not a tool-specific feature.
Pre-screening provides incremental lift (${\sim}5$pp overall, ${\sim}18$pp on Tier~1) by catching obvious formatting issues before external API calls.
We report results both with and without pre-screening to maintain transparency about which detections come from the tool versus benchmark infrastructure.

\paragraph{Calibration.}
\label{sec:calibration}
GPT-5.1 achieves the lowest ECE (0.107), followed by DOI-only (0.143).
HaRC (0.361) and verify-citations (0.317) show weaker calibration, with confidence scores poorly reflecting actual accuracy.
Confidence calibration is critical for venue deployment, where reviewers need to trust flagged entries without manual verification---a tool reporting 80\% confidence regardless of correctness provides no actionable signal to a program committee.

\paragraph{Cost--accuracy tradeoff.}
\label{sec:cost}
\cref{fig:cost} plots detection rate against API cost per entry.
DOI-only is cheapest (1 API call) but achieves only 25.6\% detection rate.
HaRC and verify-citations query multiple APIs but are bottlenecked by rate limiting, completing only 1.9\% and 6.7\% of entries respectively---making them impractical for venue-scale deployment.
GPT-5.1 achieves $3{\times}$ the detection rate of DOI-only at full coverage, though at higher per-entry cost (\$0.21/entry).
This cost--accuracy tradeoff highlights the need for tools that balance comprehensive verification with practical throughput (see \cref{app:codesign} for bibtex-updater's cost analysis).

\begin{figure}[t]
\centering
\includegraphics[width=0.65\linewidth]{figures/cost_accuracy.pdf}
\caption{Cost--accuracy tradeoff across evaluated tools. Rate-limited tools (HaRC, verify-citations) are impractical for venue-scale deployment. See \cref{app:codesign} for bibtex-updater analysis.}
\label{fig:cost}
\end{figure}

\paragraph{Synthetic vs.\ real-world representativeness.}
\label{sec:synth_vs_real}
To validate that perturbation-generated entries behave similarly to real-world hallucinations, we compare feature distributions (author count, title length, year) across the seven hallucination types that have both real-world and synthetic instances.
Kolmogorov-Smirnov tests show that 4 of 7 types are statistically indistinguishable ($p > 0.05$ on all features).
The three divergent types (\texttt{near\_miss\_title}, \texttt{plausible\_fabrication}, \texttt{chimeric\_title}) differ primarily in author count---real-world entries average 1.5--2.0 authors versus 3.3--4.7 for synthetic entries, reflecting that real LLM hallucinations tend to fabricate shorter author lists.
Title length and year distributions match across all types.
These results support using perturbation entries as controlled diagnostic tests while motivating continued expansion of the real-world sample.

\paragraph{Temporal robustness.}
\label{sec:temporal_robustness}
To stress-test GPT-5.1 on papers beyond its likely training cutoff, we construct a 60-entry probe set (30 valid entries scraped from arXiv 2026 and DBLP 2024--2025, plus 30 hallucinated variants across all tiers).
The false positive rate explodes from 17.1\% on the 2021--2023 baseline to 63.3\% on recent papers---a $3.7{\times}$ increase---while detection rate inflates to 96.7\% (from 79.7\%).
This asymmetry reveals that GPT-5.1 defaults to \texttt{HALLUCINATED} when it cannot confirm a paper from parametric memory, achieving near-perfect detection at the cost of flagging the majority of genuine recent publications.
ECE doubles (0.107 $\to$ 0.238), confirming degraded calibration.
Tier~1--2 hallucinations remain perfectly detected; the precision collapse is driven entirely by false positives on valid entries.
These results motivate including recent papers in the benchmark to expose temporal bias (\cref{app:temporal}).

\paragraph{Failure modes and improvement targets.}
\label{sec:failure}
API-based tools fail entirely on venue-level hallucinations (\texttt{preprint\_as\_published}, \texttt{wrong\_venue}) because bibliographic APIs do not reliably distinguish ``published at venue~X'' from ``available on arXiv.''
GPT-5.1 handles these types well (84\% and 83\% respectively) using parametric knowledge, but struggles with subtle perturbations: \texttt{author\_mismatch} (63\%) and \texttt{near\_miss\_title} (63\%) require exact bibliographic recall that even large LLMs lack.
These per-type gaps provide concrete targets for both tool developers (richer API metadata) and LLM-based approaches (retrieval-augmented verification).
\textsc{Hallmark}'s fine-grained analysis makes these gaps visible and measurable.

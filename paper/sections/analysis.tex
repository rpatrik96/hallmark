\section{Analysis}
\label{sec:analysis}

\paragraph{Pre-screening effect.}
\label{sec:prescreening}
\textsc{Hallmark} includes an optional pre-screening layer---DOI format validation, year-range bounds checking, and author-name heuristics---that runs before invoking external tools.
This is benchmark infrastructure available to \emph{all} baselines, not a tool-specific feature.
Pre-screening provides incremental lift (${\sim}5$pp on Tier~1 types) by catching obvious formatting issues before external API calls.
We report results both with and without pre-screening to maintain transparency about which detections come from the tool versus benchmark infrastructure.

\paragraph{Calibration.}
\label{sec:calibration}
DOI-only achieves the lowest ECE (0.111) among independent tools.
HaRC (0.361) and verify-citations (0.317) show weaker calibration, with confidence scores poorly reflecting actual accuracy.
Confidence calibration is critical for venue deployment, where reviewers need to trust flagged entries without manual verification---a tool reporting 80\% confidence regardless of correctness provides no actionable signal to a program committee.

\paragraph{Cost--accuracy tradeoff.}
\label{sec:cost}
\cref{fig:cost} plots detection rate against API cost per entry.
DOI-only is cheapest (1 API call) but achieves only 22.3\% detection rate.
HaRC and verify-citations query multiple APIs but are bottlenecked by rate limiting, completing only 2.2\% and 7.9\% of entries respectively---making them impractical for venue-scale deployment.
This cost--accuracy tradeoff highlights the need for tools that balance comprehensive verification with practical throughput (see \cref{app:codesign} for bibtex-updater's cost analysis).

\begin{figure}[t]
\centering
\includegraphics[width=0.65\linewidth]{figures/cost_accuracy.pdf}
\caption{Cost--accuracy tradeoff across evaluated tools. Rate-limited tools (HaRC, verify-citations) are impractical for venue-scale deployment. See \cref{app:codesign} for bibtex-updater analysis.}
\label{fig:cost}
\end{figure}

\paragraph{Synthetic vs.\ real-world representativeness.}
\label{sec:synth_vs_real}
To validate that perturbation-generated entries behave similarly to real-world hallucinations, we compare feature distributions (author count, title length, year) across the seven hallucination types that have both real-world and synthetic instances.
Kolmogorov-Smirnov tests show that 4 of 7 types are statistically indistinguishable ($p > 0.05$ on all features).
The three divergent types (\texttt{near\_miss\_title}, \texttt{plausible\_fabrication}, \texttt{chimeric\_title}) differ primarily in author count---real-world entries average 1.5--2.0 authors versus 3.3--4.7 for synthetic entries, reflecting that real LLM hallucinations tend to fabricate shorter author lists.
Title length and year distributions match across all types.
These results support using perturbation entries as controlled diagnostic tests while motivating continued expansion of the real-world sample.

\paragraph{Failure modes and improvement targets.}
\label{sec:failure}
No independent tool detects venue-level hallucinations (\texttt{preprint\_as\_published}, \texttt{wrong\_venue}) reliably.
Current bibliographic APIs do not reliably distinguish ``published at venue~X'' from ``available on arXiv,'' nor expose structured venue-to-paper mappings.
These gaps represent concrete improvement targets for both tool developers and database maintainers.
\textsc{Hallmark}'s per-type analysis makes these gaps visible and measurable, guiding development and advocacy for richer bibliographic APIs.

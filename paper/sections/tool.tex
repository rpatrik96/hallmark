\section{bibtex-updater: a practical verification tool}
\label{sec:tool}

Beyond the benchmark itself, we contribute \texttt{bibtex-updater}, an open-source citation verification tool designed for deployment by venues, reviewers, and authors---not to optimize benchmark scores, but to provide reliable, automated checking that integrates into existing publication workflows.

\subsection{Design goals}
\label{sec:tool_design}

The tool's design is driven by three practical requirements:
\begin{enumerate}
    \item \textbf{Zero human effort.} Verification must be fully automated---no manual review, no prompt engineering, no LLM inference costs. This rules out approaches requiring human-in-the-loop validation or expensive API calls to language models.
    \item \textbf{Workflow integration.} The tool must slot into existing pipelines: CI/CD (GitHub Actions), pre-commit hooks, Overleaf builds, and one-off command-line checks. A tool that requires a separate platform or manual invocation will not be adopted.
    \item \textbf{Graceful degradation.} When APIs are unavailable or rate-limited, the tool should return partial results rather than fail silently. Venues processing hundreds of submissions cannot tolerate flaky infrastructure.
\end{enumerate}

\subsection{Verification pipeline}
\label{sec:pipeline}

\texttt{bibtex-updater} implements a multi-stage pipeline that processes each BibTeX entry through increasingly expensive checks:

\paragraph{Pre-API validation (zero cost).}
Before any network calls, the tool checks for syntactic red flags: DOIs that fail to resolve (HEAD request to \texttt{doi.org}), future publication years, implausible dates ($< 1800$), and malformed fields. These cheap checks catch Tier~1 hallucinations without API overhead.

\paragraph{Multi-source lookup.}
The tool queries Crossref, DBLP, and Semantic Scholar using title and first-author search. Each source returns candidate records that are scored using a weighted combination of fuzzy title matching (70\%, token-sort ratio) and author Jaccard similarity (30\%). The best-scoring candidate across all sources is selected for field-by-field comparison.

\paragraph{Post-match analysis.}
Once a candidate is identified, the tool compares DOI, title, authors, year, and venue against the input entry. Venue comparison uses alias-aware matching for 17 major ML/AI venues (e.g., NeurIPS/NIPS, ICML, ICLR, CVPR), so that common name variations do not trigger false positives. A dedicated preprint detection stage queries Semantic Scholar to identify entries that claim venue publication when only an arXiv preprint exists.

\paragraph{Status assignment.}
Each entry receives one of eight status codes: \emph{verified}, \emph{not\_found}, \emph{hallucinated} (match score $< 0.50$), or specific mismatch types (\emph{title\_mismatch}, \emph{author\_mismatch}, \emph{year\_mismatch}, \emph{venue\_mismatch}, \emph{partial\_match}). The HALLMARK wrapper maps these to binary labels and confidence scores for benchmark evaluation.

\subsection{Deployment modes}
\label{sec:deployment}

The tool supports three deployment scenarios: CI/CD integration (\texttt{-{}-strict} flag exits with nonzero code on detection, gating submission workflows), pre-commit hooks (validates \texttt{.bib} files on every commit), and batch processing (concurrent workers with rate limiting for venue-scale use).
For venue-scale deployment, we recommend a two-stage workflow: run \texttt{bibtex-updater} to flag suspicious citations, then manual review of high-confidence flags (confidence $\geq 0.7$, estimated precision 0.95).
At NeurIPS scale (approximately 10,000 submissions, approximately 50 references each), this requires under 6 hours with 8 workers.
The tool is pip-installable, requires no GPU or LLM API keys, and is MIT-licensed.

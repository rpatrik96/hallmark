"""Title-match oracle baseline (diagnostic only).

This module implements a title-overlap oracle that exposes an exploitable
shortcut in HALLMARK's perturbation-based design: many HALLUCINATED entries
share their title with a VALID entry in the dev split, because hallucinations
are generated by perturbing real papers while keeping the title intact.

The oracle's logic:
- If a blind entry's title appears among VALID titles in the reference pool
  → predict HALLUCINATED (confidence 0.8)
- Otherwise → predict VALID (confidence 0.5)

**This is not a legitimate detection method.**  Titles are available to any
real tool, so title lookup is a valid strategy in production — but *using dev
VALID labels as a look-up table* constitutes label leakage when evaluating on
dev itself.  The oracle is intended as a diagnostic baseline that quantifies
the ceiling of the perturbation-structure shortcut and should be reported
alongside real baselines for transparency.

Empirical note (dev/test splits, v1.0):
- ~33 % of unique titles appear as both VALID and HALLUCINATED across splits.
- Rule applied to hidden split: F1=0.389 with perfect precision (P=1.0).
- Titles absent from any valid pool are 100 % HALLUCINATED.
"""

from __future__ import annotations

from hallmark.dataset.schema import BenchmarkEntry, BlindEntry, Prediction

# Confidence assigned to HALLUCINATED predictions (title match found).
HALLUCINATED_CONFIDENCE = 0.8

# Confidence assigned to VALID predictions (title not in reference pool).
VALID_CONFIDENCE = 0.5


def _normalise_title(title: str) -> str:
    """Lowercase and strip whitespace for robust title comparison."""
    return title.strip().lower()


def build_valid_title_set(reference_pool: list[BenchmarkEntry]) -> set[str]:
    """Extract normalised titles of all VALID entries in *reference_pool*.

    Args:
        reference_pool: BenchmarkEntry objects to mine for valid titles.
            Typically the dev split.  HALLUCINATED entries in the pool are
            ignored — only VALID entries contribute to the title set.

    Returns:
        Set of normalised title strings.
    """
    return {
        _normalise_title(entry.fields.get("title", ""))
        for entry in reference_pool
        if entry.label == "VALID" and entry.fields.get("title", "").strip()
    }


def run_title_oracle(
    entries: list[BlindEntry],
    reference_pool: list[BenchmarkEntry],
) -> list[Prediction]:
    """Run the title-match oracle on *entries* using *reference_pool*.

    For each blind entry:
    - If its title (normalised) appears in the set of VALID titles from
      *reference_pool*, predict HALLUCINATED with confidence 0.8.
    - Otherwise predict VALID with confidence 0.5.

    This oracle exploits the fact that HALLMARK hallucinations are generated
    by perturbing real (VALID) papers — so a perturbation retains the original
    title while corrupting other fields.  The oracle detects such entries by
    recognising that a title already associated with a VALID entry must belong
    to a perturbed (i.e. hallucinated) instance when it reappears.

    Args:
        entries: Blind entries to classify (no ground-truth labels visible).
        reference_pool: BenchmarkEntry objects used to build the valid-title
            look-up set.  Should be the dev split in practice.

    Returns:
        One Prediction per entry, in the same order as *entries*.
    """
    valid_titles = build_valid_title_set(reference_pool)

    predictions: list[Prediction] = []
    for entry in entries:
        title = _normalise_title(entry.fields.get("title", ""))
        if title and title in valid_titles:
            predictions.append(
                Prediction(
                    bibtex_key=entry.bibtex_key,
                    label="HALLUCINATED",
                    confidence=HALLUCINATED_CONFIDENCE,
                    reason=(
                        "[Title oracle] Title matches a VALID entry in the reference pool — "
                        "likely a perturbation of a real paper."
                    ),
                )
            )
        else:
            predictions.append(
                Prediction(
                    bibtex_key=entry.bibtex_key,
                    label="VALID",
                    confidence=VALID_CONFIDENCE,
                    reason=(
                        "[Title oracle] Title not found in the VALID reference pool — "
                        "predicted VALID by default."
                    ),
                )
            )

    return predictions
